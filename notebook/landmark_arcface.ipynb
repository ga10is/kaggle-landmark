{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "landmark_arcface.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "V6o7V7DKDAsl",
        "Iny_-XckgI5s",
        "nG4BvN4HDGcf",
        "17SUiMiNDstc",
        "b5BNMPbCDwod"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Viwm7MTHOG-x",
        "colab_type": "code",
        "outputId": "b06bd394-391a-4486-fb13-5409c319c906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BHvf6kqSOdzJ",
        "colab_type": "code",
        "outputId": "07ebdaba-9be6-4a0e-ea1a-b6f2a2d1d389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd \"/gdrive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5wCSnlo3N4Y7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "import joblib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WlhACLidNimS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (256, 256)\n",
        "INPUT = 'analysis/landmark/data/raw/'\n",
        "INDEX_PATH = INPUT + 'index.csv'\n",
        "TRAIN_PATH = INPUT + 'train.csv'\n",
        "TEST_PATH = INPUT + 'test.csv'\n",
        "SUBMIT_PATH = INPUT + 'recognition_sample_submission.csv'\n",
        "TRAIN_IMG_PATH = INPUT + 'train/'\n",
        "TEST_IMG_PATH = INPUT + 'test/'\n",
        "INDEX_IMG_PATH = INPUT + 'index/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMjYTvhlLpwB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## utility"
      ]
    },
    {
      "metadata": {
        "id": "BkejDHUXLsj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def debug_deco(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print('--start--')\n",
        "        from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "        func(*args, **kwargs)\n",
        "        print('--end--')\n",
        "    return wrapper\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "\n",
        "def split_train_valid(df, y, n_splits):    \n",
        "    # fold = StratifiedKFold(n_splits=n_splits, random_state=2019, shuffle=True)\n",
        "    fold = KFold(n_splits=n_splits, random_state=2019, shuffle=True)\n",
        "    # ignore y if KFold\n",
        "    iter_fold = fold.split(df, y)\n",
        "    idx_train, idx_valid = next(iter_fold)\n",
        "    df_train = df.iloc[idx_train]\n",
        "    df_valid = df.iloc[idx_valid]\n",
        "    \n",
        "    return df_train, df_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V6o7V7DKDAsl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## logging"
      ]
    },
    {
      "metadata": {
        "id": "3xVYg7fADCcP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "def create_logger(log_file_name):\n",
        "    logger_ = logging.getLogger('main')\n",
        "    logger_.setLevel(logging.DEBUG)\n",
        "    #fh = logging.FileHandler('whale.log')\n",
        "    fh = logging.handlers.RotatingFileHandler(log_file_name, maxBytes=100000, backupCount=8)\n",
        "    fh.setLevel(logging.DEBUG)\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    ch.setFormatter(formatter)\n",
        "    # add the handlers to the logger\n",
        "    logger_.addHandler(fh)\n",
        "    logger_.addHandler(ch)\n",
        "\n",
        "\n",
        "def get_logger():\n",
        "    return logging.getLogger('main')\n",
        "\n",
        "create_logger('landmark.log')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iny_-XckgI5s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "uq2rJ-E6gRuD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "def get_exist_image(_df, _image_folder):\n",
        "    \"\"\"\n",
        "    create dataframe of exist images in folder\n",
        "    \"\"\"\n",
        "    exist_images = get_image_ids(_image_folder)\n",
        "    df_exist = _df[_df['id'].isin(exist_images)]\n",
        "    print('exist images: %d' % len(df_exist))\n",
        "    return df_exist\n",
        "\n",
        "def assert_exist_image(df, image_folder):\n",
        "    exist_images = set(get_image_ids(image_folder))\n",
        "    df_image = set(df['id'].values)\n",
        "    print(len(exist_images))\n",
        "    print(len(df_image))\n",
        "    assert (exist_images == df_image), 'There are not all images in the \"image_folder\"'\n",
        "\n",
        "\n",
        "def get_image_ids_from_subdir(_dir_path, _sub_dir):\n",
        "    sub_dir_path = os.path.join(_dir_path, _sub_dir)\n",
        "    image_ids = [image_file.split('.')[0] for image_file in os.listdir(sub_dir_path)]\n",
        "    return image_ids\n",
        "\n",
        "\n",
        "def get_image_ids(dir_path):\n",
        "    result = []\n",
        "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
        "        futures = []\n",
        "        for sub_dir in os.listdir(dir_path):\n",
        "            futures.append(\n",
        "                executor.submit(get_image_ids_from_subdir, dir_path, sub_dir))\n",
        "\n",
        "        for future in tqdm(futures):\n",
        "            result.extend(future.result())\n",
        "    return result\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "\n",
        "def move_to_folder(dir_path):\n",
        "    for file in tqdm(os.listdir(dir_path)):\n",
        "        if(file[-4:] == '.jpg'):\n",
        "            # move image\n",
        "            sub_dir = file[0:2]\n",
        "            sub_dir_path = os.path.join(dir_path, sub_dir)\n",
        "            old_path = os.path.join(dir_path, file)\n",
        "            new_path = os.path.join(dir_path, sub_dir, file)\n",
        "            \n",
        "            os.makedirs(sub_dir_path, exist_ok=True)\n",
        "            \n",
        "            shutil.move(old_path, new_path)\n",
        "        else:\n",
        "            print('There is a file which is not image: %s' % file)\n",
        "            \n",
        "\n",
        "def init_le(_df):\n",
        "    ids = _df['landmark_id'].values.tolist()\n",
        "    le = LabelEncoder()\n",
        "    le.fit(ids)\n",
        "    return le"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uo7DwF98rCOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# move_to_folder(TEST_IMG_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nG4BvN4HDGcf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## nn"
      ]
    },
    {
      "metadata": {
        "id": "rPsB83a0AHUO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def imshow(img):\n",
        "    #print(type(img))\n",
        "    img = img * 0.23 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    #print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def save_checkpoint(state, is_best, fpath='checkpoint.pth'):\n",
        "    torch.save(state, fpath)\n",
        "    if is_best:\n",
        "        torch.save(state, 'best_model.pth')\n",
        "        \n",
        "def load_checkpoint(_model, \n",
        "                    _metric_fc,\n",
        "                    _optimizer, \n",
        "                    _scheduler, \n",
        "                    fpath):\n",
        "    checkpoint = torch.load(fpath)\n",
        "    _epoch = checkpoint['epoch']\n",
        "    _model.load_state_dict(checkpoint['state_dict'])\n",
        "    _metric_fc.load_state_dict(checkpoint['metric_fc'])\n",
        "    _optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    _scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    \n",
        "    return _epoch, _model, _metric_fc, _optimizer, _scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNimEgEnBu-j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_trnsfms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomAffine(degrees=(-30,30), shear=(-30,30)),\n",
        "    # transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "    # transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "tst_trnsfms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-glVaazC0kX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, output_neurons, n_classes, dropout_rate):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.resnet = torchvision.models.resnet34(pretrained=True)        \n",
        "        #self.resnet = torchvision.models.resnet18(pretrained=True)\n",
        "        self.norm1 = nn.BatchNorm1d(512)\n",
        "        self.drop1 = nn.Dropout(dropout_rate)\n",
        "        # FC\n",
        "        self.fc = nn.Linear(512, output_neurons)\n",
        "        self.norm2 = nn.BatchNorm1d(output_neurons)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "\n",
        "        x = self.resnet.layer1(x)\n",
        "        x = self.resnet.layer2(x)\n",
        "        x = self.resnet.layer3(x)\n",
        "        x = self.resnet.layer4(x)\n",
        "        # GAP\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.norm1(x)\n",
        "        x = self.drop1(x)\n",
        "        # FC\n",
        "        x = self.fc(x)\n",
        "        x = self.norm2(x)\n",
        "        #x = l2_norm(x)\n",
        "        return x\n",
        "    \n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, output_neurons, n_classes, dropout_rate):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.densenet_features = torchvision.models.densenet121(pretrained=True).features\n",
        "        self.norm1 = nn.BatchNorm1d(1024)\n",
        "        self.drop1 = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(1024, output_neurons)\n",
        "        self.norm2 = nn.BatchNorm1d(output_neurons)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = self.densenet_features(x)\n",
        "        x = F.relu(features, inplace=True)\n",
        "        # GAP\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).view(features.size(0), -1)\n",
        "        x = self.norm1(x)\n",
        "        x = self.drop1(x)\n",
        "        # FC\n",
        "        x = self.fc(x)\n",
        "        x = self.norm2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O20ikHNLDZzK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import math\n",
        "\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    r\"\"\"Implement of large margin arc distance: :\n",
        "        Args:\n",
        "            in_features: size of each input sample\n",
        "            out_features: size of each output sample\n",
        "            s: norm of input feature\n",
        "            m: margin\n",
        "\n",
        "            cos(theta + m)\n",
        "        \"\"\"\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, input, label=None):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        if label is None:\n",
        "            return cosine\n",
        "        \n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
        "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
        "        output *= self.s\n",
        "        #print(output[0])\n",
        "\n",
        "        return output\n",
        "    \n",
        "class FocalBinaryLoss(nn.Module):\n",
        "    def __init__(self, gamma=0):\n",
        "        super(FocalBinaryLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        p = torch.sigmoid(input)        \n",
        "        loss = torch.mean(-1 * target * torch.pow(1-p, self.gamma) * torch.log(p + 1e-10) +\n",
        "                          -1 * (1-target) * torch.pow(p, self.gamma) * torch.log(1-p + 1e-10)) * 4\n",
        "        return loss\n",
        "    \n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=0, eps=1e-7):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.eps = eps\n",
        "        self.ce = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logp = self.ce(input, target)\n",
        "        p = torch.exp(-logp)\n",
        "        loss = (1 - p) ** self.gamma * logp\n",
        "        return loss.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17SUiMiNDstc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ]
    },
    {
      "metadata": {
        "id": "LVGsNqjGDvXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "class LandmarkDataset(Dataset):\n",
        "    def __init__(self, image_folder, df, transform, is_train, le=None):\n",
        "        self.image_folder = image_folder  \n",
        "        self.transform = transform      \n",
        "        self.df = df\n",
        "        self.is_train = is_train\n",
        "        if is_train:\n",
        "            if le is None:\n",
        "                raise ValueError(\n",
        "                    'Argument \"le\" must not be None when \"is_train\" is True.')\n",
        "            self.le = le\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = '%s.jpg' % self.df.iloc[idx]['id']                   \n",
        "        img = self.__get_image(img_name)\n",
        "        label = None\n",
        "        if self.is_train:\n",
        "            id = self.df.iloc[idx]['landmark_id']\n",
        "            label = torch.tensor(self.le.transform([id]))\n",
        "        else:\n",
        "            label = -1\n",
        "        return img, label\n",
        "    \n",
        "    def __get_image(self, img_name):           \n",
        "        img = self.__load_image(img_name)\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __load_image(self, img_name):\n",
        "        \"\"\"\n",
        "        load images and bound boxing\n",
        "        \"\"\"\n",
        "        sub_folder = img_name[0:2]\n",
        "        path = os.path.join(self.image_folder, sub_folder, img_name)\n",
        "        # load images\n",
        "        img = Image.open(path).convert('RGB')               \n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b5BNMPbCDwod",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## metrics"
      ]
    },
    {
      "metadata": {
        "id": "4uWRFkm3DwlJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def GAP_vector(pred, conf, true, return_x=False):\n",
        "    '''\n",
        "    Compute Global Average Precision (aka micro AP), the metric for the\n",
        "    Google Landmark Recognition competition.\n",
        "    This function takes predictions, labels and confidence scores as vectors.\n",
        "    In both predictions and ground-truth, use None/np.nan for \"no label\".\n",
        "\n",
        "    Args:\n",
        "        pred: vector of integer-coded predictions\n",
        "        conf: vector of probability or confidence scores for pred\n",
        "        true: vector of integer-coded labels for ground truth\n",
        "        return_x: also return the data frame used in the calculation\n",
        "\n",
        "    Returns:\n",
        "        GAP score\n",
        "    '''\n",
        "    x = pd.DataFrame({'pred': pred, 'conf': conf, 'true': true})\n",
        "    x.sort_values('conf', ascending=False, inplace=True, na_position='last')\n",
        "    x['correct'] = (x.true == x.pred).astype(int)\n",
        "    x['prec_k'] = x.correct.cumsum() / (np.arange(len(x)) + 1)\n",
        "    x['term'] = x.prec_k * x.correct\n",
        "    gap = x.term.sum() / x.true.count()\n",
        "    if return_x:\n",
        "        return gap, x\n",
        "    else:\n",
        "        return gap\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "    \n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvtCrJb3lF29",
        "colab_type": "code",
        "outputId": "3b6a880f-7801-4e28-f6de-25f2615b2d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "a = torch.Tensor([\n",
        "    [0.1, 0.3, 0.2],\n",
        "    [0.2, 0.4, 0.5],\n",
        "    [2, 3, 4],\n",
        "    [3, 1, 6]\n",
        "])\n",
        "print(a.size())\n",
        "label = torch.Tensor([1, 1, 1, 1]).long()\n",
        "accuracy(a, label, topk=(1, 2))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\na = torch.Tensor([\\n    [0.1, 0.3, 0.2],\\n    [0.2, 0.4, 0.5],\\n    [2, 3, 4],\\n    [3, 1, 6]\\n])\\nprint(a.size())\\nlabel = torch.Tensor([1, 1, 1, 1]).long()\\naccuracy(a, label, topk=(1, 2))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "ngFQKdh_DwjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## train_valid"
      ]
    },
    {
      "metadata": {
        "id": "srPw7PReDwgm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch, \n",
        "          model,\n",
        "          loader,\n",
        "          metric_fc,\n",
        "          criterion,\n",
        "          optimizer):\n",
        "    loss_meter = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    \n",
        "    get_logger().info('[Start] epoch: %d' % epoch)\n",
        "    get_logger().info('lr: %f' % optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        \n",
        "    # train phase\n",
        "    model.train()\n",
        "    for i, data in enumerate(tqdm(loader)):\n",
        "        img, label = data                \n",
        "        img, label = img.cuda(), label.cuda().long()        \n",
        "        #label = label.squeeze() # (batch_size, 1) -> (batch_size,)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # forward                      \n",
        "            emb_vec = model(img)\n",
        "            logit = metric_fc(emb_vec, label)\n",
        "            loss = criterion(logit, label.squeeze())\n",
        "\n",
        "            # backward\n",
        "            loss.backward()\n",
        "            optimizer.step()            \n",
        "            \n",
        "        # measure accuracy\n",
        "        prec1, prec5 = accuracy(logit.detach(), label, topk=(1, 5))\n",
        "        loss_meter.update(loss.item(), img.size(0))\n",
        "        top1.update(prec1[0], img.size(0))\n",
        "        top5.update(prec5[0], img.size(0))\n",
        "        \n",
        "        # print\n",
        "        if i % PRINT_FREQ == 0:\n",
        "            get_logger().info('i: %d loss: %f top1: %f top5: %f' % (i, loss_meter.avg, top1.avg, top5.avg))\n",
        "    get_logger().info(\n",
        "        \"Epoch %d/%d train loss %f\" % (epoch, EPOCHS, loss_meter.avg))\n",
        "\n",
        "    # update pairs of image\n",
        "    get_logger().info('Finished updating dataset')\n",
        "    return loss_meter.avg\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def validate_arcface(model,\n",
        "                     metric_fc,\n",
        "                     loader\n",
        "                    ):\n",
        "    # validate phase\n",
        "    model.eval()    \n",
        "    with torch.no_grad():\n",
        "        proba, label = predict_proba(model, metric_fc, loader)\n",
        "        # TTA\n",
        "        '''\n",
        "        n_predict = 1\n",
        "        for i in range(n_predict-1):\n",
        "            output2, _ = predict_proba(model, metric_fc, unknown_loader)\n",
        "            print(output2[:5, :5])\n",
        "            output += output2\n",
        "        uk_output /= n_predict\n",
        "        '''        \n",
        "        # calculate accuracy\n",
        "        max_proba = np.max(proba, axis=1)\n",
        "        max_proba_idx = np.argmax(proba, axis=1)\n",
        "        acc = accuracy_score(label, max_proba_idx)\n",
        "        # calculate GAP\n",
        "        gap = GAP_vector(max_proba_idx, max_proba, label.squeeze())\n",
        "        get_logger().info(\"validate score: acc %f gap %f\" % (acc, gap))\n",
        "\n",
        "    return acc\n",
        "\n",
        "def predict_proba(model, metric_fc, loader):\n",
        "    \"\"\"\n",
        "    return numpy.ndarray of probability for each class\n",
        "    \"\"\"\n",
        "    outputs = []\n",
        "    labels = []\n",
        "    for data in tqdm(loader):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            img, label = data\n",
        "            img = img.cuda()\n",
        "            output = metric_fc(model(img))\n",
        "            outputs.append(output.detach().cpu().numpy())\n",
        "            labels.append(label.numpy())\n",
        "    outputs = np.concatenate(outputs)\n",
        "    labels = np.concatenate(labels)\n",
        "    return outputs, labels\n",
        "\n",
        "\n",
        "\n",
        "def split_dataset(dataset, steps):\n",
        "    \"\"\"\n",
        "    split Dataset by steps and create DataLoader.\n",
        "    Parameters\n",
        "    dataset: torch.utils.data.Dataset\n",
        "    steps: int\n",
        "        the number of each dataset    \n",
        "    Returns\n",
        "    list of torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "    # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "    _df = dataset.df\n",
        "    n = _df.shape[0]\n",
        "    loader_list = []\n",
        "    \n",
        "    split_indexes= np.array_split(np.arange(n), steps)\n",
        "    for split_index in split_indexes:\n",
        "        split_df = _df.iloc[split_index]\n",
        "        split_dataset = LandmarkDataset(dataset.image_folder, \n",
        "                                        split_df, \n",
        "                                        dataset.transform, \n",
        "                                        is_train=False)\n",
        "        split_loader = DataLoader(split_dataset,\n",
        "                          batch_size=BATCH_SIZE_TRAIN,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=False,\n",
        "                          shuffle=False\n",
        "                         )\n",
        "        loader_list.append(split_loader)\n",
        "    return loader_list\n",
        "\n",
        "    \n",
        "\n",
        "def make_df(df_org, labels, confidences):\n",
        "    \"\"\"\n",
        "    make dataframe for submission\n",
        "    df_org: pd.DataFrame of shape = [n_samples, more than 1]\n",
        "        dataframe which have id column\n",
        "    labels: ndarray of shape = [n_samples]\n",
        "        array of label(number)\n",
        "    confidences: ndarray of shape = [n_samples]\n",
        "        array of confidence(float)\n",
        "    Returns\n",
        "    pd.DataFrame of shape = [n_samples, 2]\n",
        "        the dataframe has 'id', 'landmarks' columns.\n",
        "    \"\"\"\n",
        "    # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "    new_df = pd.DataFrame()\n",
        "    new_df['id'] = df_org['id']\n",
        "    new_df['label'] = labels.astype(str)\n",
        "    new_df['confidence'] = confidences.astype(str)\n",
        "    new_df['landmarks'] = new_df['label'] + ' ' + new_df['confidence']\n",
        "    del new_df['label'], new_df['confidence']\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def predict_label(model, metric_fc, test_dataset, label_encoder):\n",
        "    submit_file = 'submit_landmark.csv'\n",
        "    \n",
        "    # split df in test_dataset and make loader\n",
        "    loaders = split_dataset(test_dataset, 10)\n",
        "    \n",
        "    # write the header of a submission table\n",
        "    df_header = pd.DataFrame(columns=['id', 'landmarks'])\n",
        "    df_header.to_csv(submit_file, index=False)\n",
        "    \n",
        "    # prediction phase\n",
        "    for i, loader in enumerate(loaders):\n",
        "        get_logger().info('prediction %d / %d' % (i+1, len(loaders)))\n",
        "        model.eval()\n",
        "        with torch.no_grad():    \n",
        "            proba, _ = predict_proba(model, metric_fc, loaders[i])\n",
        "            max_proba = np.max(proba, axis=1)\n",
        "            max_proba_idx = np.argmax(proba, axis=1)\n",
        "            labels = label_encoder.inverse_transform(max_proba_idx)\n",
        "\n",
        "            df_submit = make_df(loader.dataset.df, labels, max_proba)\n",
        "\n",
        "        # write result in appending mode\n",
        "        df_submit.to_csv(submit_file, index=False, header=False, mode='a')\n",
        "        \n",
        "    get_logger().info(\"created submission file\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPUzcAtLDwd7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## main"
      ]
    },
    {
      "metadata": {
        "id": "DKnEDDIBascd",
        "colab_type": "code",
        "outputId": "1bc918aa-89cc-4d79-f268-64b6791728eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE_TRAIN = 150\n",
        "NUM_WORKERS = 8\n",
        "EPOCHS = 10\n",
        "PRINT_FREQ = 100\n",
        "latent_dim = 512\n",
        "get_logger().info('batch size: %d' % BATCH_SIZE_TRAIN)\n",
        "get_logger().info('epochs: %d' % EPOCHS)\n",
        "get_logger().info('latent_dim: %d' % latent_dim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]2019-04-24 15:29:30,143:main:batch size: 150\n",
            "[INFO]2019-04-24 15:29:30,153:main:epochs: 10\n",
            "[INFO]2019-04-24 15:29:30,156:main:latent_dim: 512\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "11g95PtYVs8L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### train"
      ]
    },
    {
      "metadata": {
        "id": "JRQ04fG0Dwbc",
        "colab_type": "code",
        "outputId": "8d410f5e-9565-4823-e4d6-e5b0801be0e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "# load train data\n",
        "df_train = pd.read_csv(TRAIN_PATH, dtype={'id': 'object'})\n",
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>landmark_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>97c0a12e07ae8dd5</td>\n",
              "      <td>http://lh4.ggpht.com/-f8xYA5l4apw/RSziSQVaABI/...</td>\n",
              "      <td>6347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>650c989dd3493748</td>\n",
              "      <td>https://lh5.googleusercontent.com/-PUnMrX7oOyA...</td>\n",
              "      <td>12519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>05e63ca9b2cde1f4</td>\n",
              "      <td>http://mw2.google.com/mw-panoramio/photos/medi...</td>\n",
              "      <td>264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>08672eddcb2b7c93</td>\n",
              "      <td>http://lh3.ggpht.com/-9fgSxDYwhHA/SMvGEoltKTI/...</td>\n",
              "      <td>13287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fc49cb32ef7f1e89</td>\n",
              "      <td>http://lh6.ggpht.com/-UGAXxvPbr98/S-jGZbyMIPI/...</td>\n",
              "      <td>4018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                                url  \\\n",
              "0  97c0a12e07ae8dd5  http://lh4.ggpht.com/-f8xYA5l4apw/RSziSQVaABI/...   \n",
              "1  650c989dd3493748  https://lh5.googleusercontent.com/-PUnMrX7oOyA...   \n",
              "2  05e63ca9b2cde1f4  http://mw2.google.com/mw-panoramio/photos/medi...   \n",
              "3  08672eddcb2b7c93  http://lh3.ggpht.com/-9fgSxDYwhHA/SMvGEoltKTI/...   \n",
              "4  fc49cb32ef7f1e89  http://lh6.ggpht.com/-UGAXxvPbr98/S-jGZbyMIPI/...   \n",
              "\n",
              "  landmark_id  \n",
              "0        6347  \n",
              "1       12519  \n",
              "2         264  \n",
              "3       13287  \n",
              "4        4018  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "QYHywf19jfwx",
        "colab_type": "code",
        "outputId": "70937d1c-5f35-44a2-9eae-633ffb41f5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "# create label encoder\n",
        "# must be init_le() before get_exist_image()\n",
        "label_encoder = init_le(df_train)\n",
        "joblib.dump(label_encoder, 'le.pkl')\n",
        "\n",
        "# use landmark_id which have more than 1 image\n",
        "df_train = get_exist_image(df_train, TRAIN_IMG_PATH)\n",
        "id_count = df_train['landmark_id'].value_counts()\n",
        "s_count = df_train['landmark_id'].map(id_count)\n",
        "df_train = df_train[s_count > 1]\n",
        "print('more than 1 landmark_id: %d, images: %d' % ((id_count > 1).sum(), df_train.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 256/256 [00:02<00:00, 123.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "exist images: 340748\n",
            "more than 1 landmark_id: 11257, images: 338428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nh9XJ9nSxPK-",
        "colab_type": "code",
        "outputId": "e95b5411-210c-4334-a0e6-ae5356fdf571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# train validate split\n",
        "df_trn, df_val = split_train_valid(df_train, label_encoder.transform(df_train['landmark_id'].values), 30)\n",
        "get_logger().info('train num: %d, valid num: %d' % (len(df_trn), len(df_val)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]2019-04-24 15:29:35,820:main:train num: 327147, valid num: 11281\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "UpFXwjrxDwY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize Dataset and DataLoader\n",
        "train_dataset = LandmarkDataset(TRAIN_IMG_PATH, df_trn, \n",
        "                                trn_trnsfms, is_train=True, le=label_encoder)\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=BATCH_SIZE_TRAIN,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=True,\n",
        "                          shuffle=True\n",
        "                         )\n",
        "valid_dataset = LandmarkDataset(TRAIN_IMG_PATH, df_val, \n",
        "                                tst_trnsfms, is_train=True, le=label_encoder)\n",
        "valid_loader = DataLoader(valid_dataset,\n",
        "                          batch_size=BATCH_SIZE_TRAIN,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=False,\n",
        "                          shuffle=False\n",
        "                         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VFP18BuDwQx",
        "colab_type": "code",
        "outputId": "885d65d2-7cf4-4901-a749-445985d85c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset.df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(327147, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "450xobzQoojo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_classes = len(label_encoder.classes_)\n",
        "#model = DenseNet(output_neurons=latent_dim, n_classes=len(le.classes_),  dropout_rate=0.5).cuda()    \n",
        "model = ResNet(output_neurons=latent_dim, n_classes=n_classes, dropout_rate=0.5).cuda()    \n",
        "metric_fc = ArcMarginProduct(latent_dim, n_classes, s=60, m=0.5, easy_margin=False).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = FocalLoss(gamma=2)\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = optim.SGD([{'params':model.parameters()}, {'params': metric_fc.parameters()}], \n",
        "                      lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler_step = EPOCHS\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, eta_min=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FXFjd-6xtEmq",
        "colab_type": "code",
        "outputId": "e47d5b8a-c385-4ecf-b446-989b6353bb79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch+1, EPOCHS+1):\n",
        "    scheduler.step()\n",
        "    \n",
        "    epoch_loss = train(epoch, model, train_loader, metric_fc, criterion, optimizer)\n",
        "    \n",
        "    validate_arcface(model, metric_fc, valid_loader)\n",
        "    \n",
        "    save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'metric_fc': metric_fc.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict()\n",
        "    }, True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]2019-04-24 15:29:40,395:main:[Start] epoch: 1\n",
            "[INFO]2019-04-24 15:29:40,400:main:lr: 0.001000\n",
            "  0%|          | 0/2180 [00:00<?, ?it/s][INFO]2019-04-24 15:30:26,251:main:i: 0 loss: 41.924927 top1: 0.000000 top5: 0.000000\n",
            "  5%|▍         | 100/2180 [10:07<3:16:13,  5.66s/it][INFO]2019-04-24 15:39:54,111:main:i: 100 loss: 37.365409 top1: 3.227723 top5: 3.656766\n",
            "  9%|▉         | 200/2180 [19:46<2:12:26,  4.01s/it][INFO]2019-04-24 15:49:31,677:main:i: 200 loss: 35.099553 top1: 5.737977 top5: 6.417911\n",
            " 14%|█▍        | 300/2180 [29:01<3:25:55,  6.57s/it][INFO]2019-04-24 15:58:48,343:main:i: 300 loss: 33.770880 top1: 7.477298 top5: 8.394241\n",
            " 18%|█▊        | 400/2180 [39:35<2:07:57,  4.31s/it][INFO]2019-04-24 16:09:38,945:main:i: 400 loss: 32.685667 top1: 8.989194 top5: 10.053201\n",
            " 23%|██▎       | 500/2180 [49:50<3:55:20,  8.41s/it][INFO]2019-04-24 16:19:33,202:main:i: 500 loss: 31.941416 top1: 10.021291 top5: 11.123087\n",
            " 28%|██▊       | 600/2180 [58:40<1:17:19,  2.94s/it][INFO]2019-04-24 16:28:23,072:main:i: 600 loss: 31.258429 top1: 11.039378 top5: 12.194120\n",
            " 32%|███▏      | 700/2180 [1:08:17<2:43:57,  6.65s/it][INFO]2019-04-24 16:37:59,639:main:i: 700 loss: 30.702418 top1: 11.771754 top5: 12.960532\n",
            " 37%|███▋      | 800/2180 [1:17:07<1:13:25,  3.19s/it][INFO]2019-04-24 16:46:49,782:main:i: 800 loss: 30.210499 top1: 12.463587 top5: 13.662089\n",
            " 41%|████▏     | 900/2180 [1:26:23<2:44:22,  7.70s/it][INFO]2019-04-24 16:56:06,118:main:i: 900 loss: 29.767090 top1: 13.044024 top5: 14.282649\n",
            " 46%|████▌     | 1000/2180 [1:35:14<1:02:08,  3.16s/it][INFO]2019-04-24 17:04:56,399:main:i: 1000 loss: 29.377771 top1: 13.571096 top5: 14.844489\n",
            " 50%|█████     | 1100/2180 [1:45:04<2:01:05,  6.73s/it][INFO]2019-04-24 17:14:47,055:main:i: 1100 loss: 29.019867 top1: 14.038147 top5: 15.363004\n",
            " 55%|█████▌    | 1200/2180 [1:54:03<42:39,  2.61s/it][INFO]2019-04-24 17:24:14,170:main:i: 1200 loss: 28.699718 top1: 14.394116 top5: 15.756869\n",
            " 60%|█████▉    | 1300/2180 [2:03:52<1:30:47,  6.19s/it][INFO]2019-04-24 17:33:34,914:main:i: 1300 loss: 28.358010 top1: 14.862413 top5: 16.278248\n",
            " 64%|██████▍   | 1400/2180 [2:13:01<33:42,  2.59s/it][INFO]2019-04-24 17:43:08,409:main:i: 1400 loss: 28.053029 top1: 15.262907 top5: 16.727575\n",
            " 69%|██████▉   | 1500/2180 [2:22:12<1:00:42,  5.36s/it][INFO]2019-04-24 17:51:55,140:main:i: 1500 loss: 27.762004 top1: 15.651787 top5: 17.159227\n",
            " 73%|███████▎  | 1600/2180 [2:30:46<24:29,  2.53s/it][INFO]2019-04-24 18:00:51,373:main:i: 1600 loss: 27.480633 top1: 16.009161 top5: 17.565271\n",
            " 78%|███████▊  | 1700/2180 [2:40:36<41:05,  5.14s/it][INFO]2019-04-24 18:10:18,630:main:i: 1700 loss: 27.215890 top1: 16.368019 top5: 17.964727\n",
            " 83%|████████▎ | 1800/2180 [2:49:24<16:02,  2.53s/it][INFO]2019-04-24 18:19:34,156:main:i: 1800 loss: 26.970137 top1: 16.695910 top5: 18.339071\n",
            " 87%|████████▋ | 1900/2180 [2:58:47<24:27,  5.24s/it][INFO]2019-04-24 18:28:30,240:main:i: 1900 loss: 26.728435 top1: 16.994915 top5: 18.699631\n",
            " 92%|█████████▏| 2000/2180 [3:07:39<07:38,  2.55s/it][INFO]2019-04-24 18:38:01,115:main:i: 2000 loss: 26.497939 top1: 17.288689 top5: 19.041479\n",
            " 96%|█████████▋| 2100/2180 [3:16:43<06:02,  4.53s/it][INFO]2019-04-24 18:46:25,842:main:i: 2100 loss: 26.264512 top1: 17.617325 top5: 19.414564\n",
            "100%|██████████| 2180/2180 [3:23:16<00:00,  4.07s/it]\n",
            "[INFO]2019-04-24 18:52:57,500:main:Epoch 1/10 train loss 26.097914\n",
            "[INFO]2019-04-24 18:52:57,502:main:Finished updating dataset\n",
            "100%|██████████| 76/76 [01:16<00:00,  1.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m<ipython-input-16-d64da284e671>\u001b[0m(69)\u001b[0;36mvalidate_arcface\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     67 \u001b[0;31m        \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mPdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     68 \u001b[0;31m        \u001b[0;31m# calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 69 \u001b[0;31m        \u001b[0mmax_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     70 \u001b[0;31m        \u001b[0mmax_proba_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     71 \u001b[0;31m        \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_proba_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "--KeyboardInterrupt--\n",
            "--KeyboardInterrupt--\n",
            "--KeyboardInterrupt--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xc807qKVw6hU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### prediction"
      ]
    },
    {
      "metadata": {
        "id": "7vpb33qAj3MC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_all = pd.read_csv(TEST_PATH, dtype={'id': 'object'})\n",
        "df_test_all.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbK7LyNrj8t4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test = get_exist_image(df_test_all, TEST_IMG_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kYrRy4RScvlS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dataset = LandmarkDataset(TEST_IMG_PATH, df_test, tst_trnsfms, is_train=False)\n",
        "label_encoder = joblib.load('analysis/landmark/models/20190423/le.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Gvqw8VvhWtO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_TBDnxiGvDd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_classes = len(label_encoder.classes_)\n",
        "#model = DenseNet(output_neurons=latent_dim, n_classes=len(le.classes_),  dropout_rate=0.5).cuda()    \n",
        "model = ResNet(output_neurons=latent_dim, n_classes=n_classes, dropout_rate=0.5).cuda()    \n",
        "metric_fc = ArcMarginProduct(latent_dim, n_classes, s=60, m=0.5, easy_margin=False).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = FocalLoss(gamma=2)\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = optim.SGD([{'params':model.parameters()}, {'params': metric_fc.parameters()}], \n",
        "                      lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler_step = 200\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, eta_min=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIQ0_AanWPE7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create model object before following statement\n",
        "start_epoch, model, metric_fc, optimizer, scheduler = load_checkpoint(model, \n",
        "                                                                      metric_fc,\n",
        "                                                                      optimizer,\n",
        "                                                                      scheduler, \n",
        "                                                                      'analysis/landmark/models/20190423/best_model.pth')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DxxcCuStBbIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_label(model, metric_fc, test_dataset, label_encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nci2ykdwExk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub = pd.read_csv('submit_landmark.csv', dtype={'id': 'object'})\n",
        "df_sub.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5aTEO9kcipx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_all.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDEx58-1cshM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHg0WjmhgP0F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub_sample = pd.read_csv(SUBMIT_PATH, dtype={'id': 'object'})\n",
        "del df_sub_sample['landmarks']\n",
        "df_sub_sample.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B82D2mJphaHI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub_sample.head(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "epJCM5ZLhiaf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_all.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nfOhWMPpnvog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2 = df_sub_sample.merge(df_sub, how='left', on='id')[['id', 'landmarks']]\n",
        "df_sub2.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wG-pXdfxn-_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-gb8p20oCf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2['landmarks'].fillna('', inplace=True)\n",
        "df_sub2['landmarks'].isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kyl1lq6NoS70",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2.to_csv('submit_landmark2.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t513LvlOpHT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}