{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "landmark_arcface.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "FMjYTvhlLpwB",
        "V6o7V7DKDAsl",
        "Iny_-XckgI5s",
        "17SUiMiNDstc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Viwm7MTHOG-x",
        "colab_type": "code",
        "outputId": "efe78272-60d2-4612-f309-d2fbba1bcb0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BHvf6kqSOdzJ",
        "colab_type": "code",
        "outputId": "9d68b215-57fa-44c5-da93-607b13844b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd \"/gdrive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5wCSnlo3N4Y7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "import joblib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WlhACLidNimS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (256, 256)\n",
        "INPUT = 'analysis/landmark/data/raw/'\n",
        "INDEX_PATH = INPUT + 'index.csv'\n",
        "TRAIN_PATH = INPUT + 'train.csv'\n",
        "TEST_PATH = INPUT + 'test.csv'\n",
        "TRAIN_IMG_PATH = INPUT + 'train/'\n",
        "TEST_IMG_PATH = INPUT + 'test/'\n",
        "INDEX_IMG_PATH = INPUT + 'index/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMjYTvhlLpwB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## utility"
      ]
    },
    {
      "metadata": {
        "id": "BkejDHUXLsj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def debug_deco(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print('--start--')\n",
        "        from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "        func(*args, **kwargs)\n",
        "        print('--end--')\n",
        "    return wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V6o7V7DKDAsl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## logging"
      ]
    },
    {
      "metadata": {
        "id": "3xVYg7fADCcP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "def create_logger(log_file_name):\n",
        "    logger_ = logging.getLogger('main')\n",
        "    logger_.setLevel(logging.DEBUG)\n",
        "    #fh = logging.FileHandler('whale.log')\n",
        "    fh = logging.handlers.RotatingFileHandler(log_file_name, maxBytes=100000, backupCount=8)\n",
        "    fh.setLevel(logging.DEBUG)\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    ch.setFormatter(formatter)\n",
        "    # add the handlers to the logger\n",
        "    logger_.addHandler(fh)\n",
        "    logger_.addHandler(ch)\n",
        "\n",
        "\n",
        "def get_logger():\n",
        "    return logging.getLogger('main')\n",
        "\n",
        "create_logger('landmark.log')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iny_-XckgI5s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "uq2rJ-E6gRuD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "def get_exist_image(_df, _image_folder):\n",
        "    \"\"\"\n",
        "    create dataframe of exist images in folder\n",
        "    \"\"\"\n",
        "    exist_images = get_image_ids(_image_folder)\n",
        "    df_exist = _df[_df['id'].isin(exist_images)]\n",
        "    print('exist images: %d' % len(exist_images))\n",
        "    return df_exist\n",
        "\n",
        "def assert_exist_image(df, image_folder):\n",
        "    exist_images = set(get_image_ids(image_folder))\n",
        "    df_image = set(df['id'].values)\n",
        "    print(len(exist_images))\n",
        "    print(len(df_image))\n",
        "    assert (exist_images == df_image), 'There are not all images in the \"image_folder\"'\n",
        "\n",
        "\n",
        "def get_image_ids_from_subdir(_dir_path, _sub_dir):\n",
        "    sub_dir_path = os.path.join(_dir_path, _sub_dir)\n",
        "    image_ids = [image_file.split('.')[0] for image_file in os.listdir(sub_dir_path)]\n",
        "    return image_ids\n",
        "\n",
        "\n",
        "def get_image_ids(dir_path):\n",
        "    result = []\n",
        "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
        "        futures = []\n",
        "        for sub_dir in os.listdir(dir_path):\n",
        "            futures.append(\n",
        "                executor.submit(get_image_ids_from_subdir, dir_path, sub_dir))\n",
        "\n",
        "        for future in tqdm(futures):\n",
        "            result.extend(future.result())\n",
        "    return result\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "\n",
        "def move_to_folder(dir_path):\n",
        "    for file in tqdm(os.listdir(dir_path)):\n",
        "        if(file[-4:] == '.jpg'):\n",
        "            # move image\n",
        "            sub_dir = file[0:2]\n",
        "            sub_dir_path = os.path.join(dir_path, sub_dir)\n",
        "            old_path = os.path.join(dir_path, file)\n",
        "            new_path = os.path.join(dir_path, sub_dir, file)\n",
        "            \n",
        "            os.makedirs(sub_dir_path, exist_ok=True)\n",
        "            \n",
        "            shutil.move(old_path, new_path)\n",
        "        else:\n",
        "            print('There is a file which is not image: %s' % file)\n",
        "            \n",
        "\n",
        "def init_le(_df):\n",
        "    ids = _df['landmark_id'].values.tolist()\n",
        "    le = LabelEncoder()\n",
        "    le.fit(ids)\n",
        "    return le"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uo7DwF98rCOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# move_to_folder(TEST_IMG_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nG4BvN4HDGcf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## nn"
      ]
    },
    {
      "metadata": {
        "id": "rPsB83a0AHUO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def imshow(img):\n",
        "    #print(type(img))\n",
        "    img = img * 0.23 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    #print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def save_checkpoint(state, is_best, fpath='checkpoint.pth'):\n",
        "    torch.save(state, fpath)\n",
        "    if is_best:\n",
        "        torch.save(state, 'best_model.pth')\n",
        "        \n",
        "def load_checkpoint(_model, \n",
        "                    _metric_fc,\n",
        "                    _optimizer, \n",
        "                    _scheduler, \n",
        "                    fpath):\n",
        "    checkpoint = torch.load(fpath)\n",
        "    _epoch = checkpoint['epoch']\n",
        "    _model.load_state_dict(checkpoint['state_dict'])\n",
        "    _metric_fc.load_state_dict(checkpoint['metric_fc'])\n",
        "    _optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    _scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    \n",
        "    return _epoch, _model, _metric_fc, _optimizer, _scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNimEgEnBu-j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_trnsfms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomAffine(degrees=(-30,30), shear=(-30,30)),\n",
        "    # transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "    # transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "tst_trnsfms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-glVaazC0kX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, output_neurons, n_classes, dropout_rate):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.resnet = torchvision.models.resnet34(pretrained=True)        \n",
        "        #self.resnet = torchvision.models.resnet18(pretrained=True)\n",
        "        self.norm1 = nn.BatchNorm1d(512)\n",
        "        self.drop1 = nn.Dropout(dropout_rate)\n",
        "        # FC\n",
        "        self.fc = nn.Linear(512, output_neurons)\n",
        "        self.norm2 = nn.BatchNorm1d(output_neurons)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "\n",
        "        x = self.resnet.layer1(x)\n",
        "        x = self.resnet.layer2(x)\n",
        "        x = self.resnet.layer3(x)\n",
        "        x = self.resnet.layer4(x)\n",
        "        # GAP\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.norm1(x)\n",
        "        x = self.drop1(x)\n",
        "        # FC\n",
        "        x = self.fc(x)\n",
        "        x = self.norm2(x)\n",
        "        #x = l2_norm(x)\n",
        "        return x\n",
        "    \n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, output_neurons, n_classes, dropout_rate):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.densenet_features = torchvision.models.densenet121(pretrained=True).features\n",
        "        self.norm1 = nn.BatchNorm1d(1024)\n",
        "        self.drop1 = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(1024, output_neurons)\n",
        "        self.norm2 = nn.BatchNorm1d(output_neurons)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = self.densenet_features(x)\n",
        "        x = F.relu(features, inplace=True)\n",
        "        # GAP\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).view(features.size(0), -1)\n",
        "        x = self.norm1(x)\n",
        "        x = self.drop1(x)\n",
        "        # FC\n",
        "        x = self.fc(x)\n",
        "        x = self.norm2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O20ikHNLDZzK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import math\n",
        "\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    r\"\"\"Implement of large margin arc distance: :\n",
        "        Args:\n",
        "            in_features: size of each input sample\n",
        "            out_features: size of each output sample\n",
        "            s: norm of input feature\n",
        "            m: margin\n",
        "\n",
        "            cos(theta + m)\n",
        "        \"\"\"\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, input, label=None):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        if label is None:\n",
        "            return cosine\n",
        "        \n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
        "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
        "        output *= self.s\n",
        "        #print(output[0])\n",
        "\n",
        "        return output\n",
        "    \n",
        "class FocalBinaryLoss(nn.Module):\n",
        "    def __init__(self, gamma=0):\n",
        "        super(FocalBinaryLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        p = torch.sigmoid(input)        \n",
        "        loss = torch.mean(-1 * target * torch.pow(1-p, self.gamma) * torch.log(p + 1e-10) +\n",
        "                          -1 * (1-target) * torch.pow(p, self.gamma) * torch.log(1-p + 1e-10)) * 4\n",
        "        return loss\n",
        "    \n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=0, eps=1e-7):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.eps = eps\n",
        "        self.ce = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logp = self.ce(input, target)\n",
        "        p = torch.exp(-logp)\n",
        "        loss = (1 - p) ** self.gamma * logp\n",
        "        return loss.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17SUiMiNDstc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ]
    },
    {
      "metadata": {
        "id": "LVGsNqjGDvXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "class LandmarkDataset(Dataset):\n",
        "    def __init__(self, image_folder, df, transform, is_train, le=None):\n",
        "        self.image_folder = image_folder  \n",
        "        self.transform = transform      \n",
        "        self.df = df\n",
        "        self.is_train = is_train\n",
        "        if is_train:\n",
        "            if le is None:\n",
        "                raise ValueError(\n",
        "                    'Argument \"le\" must not be None when \"is_train\" is True.')\n",
        "            self.le = le\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = '%s.jpg' % self.df.iloc[idx]['id']                   \n",
        "        img = self.__get_image(img_name)\n",
        "        label = None\n",
        "        if self.is_train:\n",
        "            id = self.df.iloc[idx]['landmark_id']\n",
        "            label = torch.tensor(self.le.transform([id]))\n",
        "        else:\n",
        "            label = -1\n",
        "        return img, label\n",
        "    \n",
        "    def __get_image(self, img_name):           \n",
        "        img = self.__load_image(img_name)\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __load_image(self, img_name):\n",
        "        \"\"\"\n",
        "        load images and bound boxing\n",
        "        \"\"\"\n",
        "        sub_folder = img_name[0:2]\n",
        "        path = os.path.join(self.image_folder, sub_folder, img_name)\n",
        "        # load images\n",
        "        img = Image.open(path).convert('RGB')               \n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b5BNMPbCDwod",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## metrics"
      ]
    },
    {
      "metadata": {
        "id": "4uWRFkm3DwlJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def GAP_vector(pred, conf, true, return_x=False):\n",
        "    '''\n",
        "    Compute Global Average Precision (aka micro AP), the metric for the\n",
        "    Google Landmark Recognition competition.\n",
        "    This function takes predictions, labels and confidence scores as vectors.\n",
        "    In both predictions and ground-truth, use None/np.nan for \"no label\".\n",
        "\n",
        "    Args:\n",
        "        pred: vector of integer-coded predictions\n",
        "        conf: vector of probability or confidence scores for pred\n",
        "        true: vector of integer-coded labels for ground truth\n",
        "        return_x: also return the data frame used in the calculation\n",
        "\n",
        "    Returns:\n",
        "        GAP score\n",
        "    '''\n",
        "    x = pd.DataFrame({'pred': pred, 'conf': conf, 'true': true})\n",
        "    x.sort_values('conf', ascending=False, inplace=True, na_position='last')\n",
        "    x['correct'] = (x.true == x.pred).astype(int)\n",
        "    x['prec_k'] = x.correct.cumsum() / (np.arange(len(x)) + 1)\n",
        "    x['term'] = x.prec_k * x.correct\n",
        "    gap = x.term.sum() / x.true.count()\n",
        "    if return_x:\n",
        "        return gap, x\n",
        "    else:\n",
        "        return gap\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "    \n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvtCrJb3lF29",
        "colab_type": "code",
        "outputId": "37e5b194-eb0b-4f53-e9d6-f029bf8e5dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "a = torch.Tensor([\n",
        "    [0.1, 0.3, 0.2],\n",
        "    [0.2, 0.4, 0.5],\n",
        "    [2, 3, 4],\n",
        "    [3, 1, 6]\n",
        "])\n",
        "print(a.size())\n",
        "label = torch.Tensor([1, 1, 1, 1]).long()\n",
        "accuracy(a, label, topk=(1, 2))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\na = torch.Tensor([\\n    [0.1, 0.3, 0.2],\\n    [0.2, 0.4, 0.5],\\n    [2, 3, 4],\\n    [3, 1, 6]\\n])\\nprint(a.size())\\nlabel = torch.Tensor([1, 1, 1, 1]).long()\\naccuracy(a, label, topk=(1, 2))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "ngFQKdh_DwjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## train_valid"
      ]
    },
    {
      "metadata": {
        "id": "srPw7PReDwgm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch, \n",
        "          model,\n",
        "          loader,\n",
        "          metric_fc,\n",
        "          criterion,\n",
        "          optimizer):\n",
        "    loss_meter = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    \n",
        "    get_logger().info('[Start] epoch: %d' % epoch)\n",
        "    get_logger().info('lr: %f' % optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        \n",
        "    # train phase\n",
        "    model.train()\n",
        "    for i, data in enumerate(tqdm(loader)):\n",
        "        img, label = data                \n",
        "        img, label = img.cuda(), label.cuda().long()        \n",
        "        #label = label.squeeze() # (batch_size, 1) -> (batch_size,)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # forward                      \n",
        "            emb_vec = model(img)\n",
        "            logit = metric_fc(emb_vec, label)\n",
        "            loss = criterion(logit, label.squeeze())\n",
        "\n",
        "            # backward\n",
        "            loss.backward()\n",
        "            optimizer.step()            \n",
        "            \n",
        "        # measure accuracy\n",
        "        prec1, prec5 = accuracy(logit.detach(), label, topk=(1, 5))\n",
        "        loss_meter.update(loss.item(), img.size(0))\n",
        "        top1.update(prec1[0], img.size(0))\n",
        "        top5.update(prec5[0], img.size(0))\n",
        "        \n",
        "        # print\n",
        "        if i % PRINT_FREQ == 0:\n",
        "            get_logger().info('i: %d loss: %f top1: %f top5: %f' % (i, loss_meter.avg, top1.avg, top5.avg))\n",
        "    get_logger().info(\n",
        "        \"Epoch %d/%d train loss %f\" % (epoch, EPOCHS, loss_meter.avg))\n",
        "\n",
        "    # update pairs of image\n",
        "    get_logger().info('Finished updating dataset')\n",
        "    return loss_meter.avg\n",
        "\n",
        "\n",
        "def validate_arcface(model,\n",
        "                     metric_fc,\n",
        "                     unknown_loader,\n",
        "                     label_encoder\n",
        "                    ):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    # validate phase\n",
        "    model.eval()    \n",
        "    with torch.no_grad():       \n",
        "        # predict latent features of Unknown whales\n",
        "        uk_output = predict_arcface(model, metric_fc, unknown_loader)        \n",
        "        n_predict = 1\n",
        "        for i in range(n_predict-1):\n",
        "            uk_output2 = predict_arcface(model, metric_fc, unknown_loader)\n",
        "            print(uk_output2[:5, :5])\n",
        "            uk_output += uk_output2\n",
        "        uk_output /= n_predict\n",
        "        \n",
        "        df_known = make_df(label_encoder)\n",
        "        mat_distance = pd.DataFrame(data=uk_output.T, \n",
        "                                    columns=unknown_loader.dataset.df['Image'].values, \n",
        "                                    index=df_known['Image'].values)\n",
        "\n",
        "        # compute map@5\n",
        "        score = compute_map5(mat_distance, df_known, unknown_loader.dataset.df)        \n",
        "        #acc = accuracy(uclasses, unknown_loader.dataset.df['Id'].values)\n",
        "        get_logger().info(\"validate score %f\" % (score))\n",
        "\n",
        "    return score, mat_distance\n",
        "\n",
        "def predict_proba(model, metric_fc, loader):\n",
        "    \"\"\"\n",
        "    return Tensor of probability for each class\n",
        "    \"\"\"\n",
        "    outputs = []\n",
        "    for data in tqdm(loader):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            img, _ = data\n",
        "            img = img.cuda()        \n",
        "            output = metric_fc(model(img))\n",
        "            outputs.append(output.detach().cpu().numpy())    \n",
        "    outputs = np.concatenate(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "def split_dataset(dataset, steps):\n",
        "    \"\"\"\n",
        "    split Dataset by steps and create DataLoader.\n",
        "    Parameters\n",
        "    dataset: torch.utils.data.Dataset\n",
        "    steps: int\n",
        "        the number of each dataset    \n",
        "    Returns\n",
        "    list of torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "    # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "    _df = dataset.df\n",
        "    n = _df.shape[0]\n",
        "    loader_list = []\n",
        "    \n",
        "    split_indexes= np.array_split(np.arange(n), steps)\n",
        "    for split_index in split_indexes:\n",
        "        split_df = _df.iloc[split_index]\n",
        "        split_dataset = LandmarkDataset(dataset.image_folder, \n",
        "                                        split_df, \n",
        "                                        dataset.transform, \n",
        "                                        is_train=False)\n",
        "        split_loader = DataLoader(split_dataset,\n",
        "                          batch_size=BATCH_SIZE_TRAIN,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=False,\n",
        "                          shuffle=False\n",
        "                         )\n",
        "        loader_list.append(split_loader)\n",
        "    return loader_list\n",
        "\n",
        "    \n",
        "\n",
        "def make_df(df_org, labels, confidences):\n",
        "    \"\"\"\n",
        "    make dataframe for submission\n",
        "    df_org: pd.DataFrame of shape = [n_samples, more than 1]\n",
        "        dataframe which have id column\n",
        "    labels: ndarray of shape = [n_samples]\n",
        "        array of label(number)\n",
        "    confidences: ndarray of shape = [n_samples]\n",
        "        array of confidence(float)\n",
        "    Returns\n",
        "    pd.DataFrame of shape = [n_samples, 2]\n",
        "        the dataframe has 'id', 'landmarks' columns.\n",
        "    \"\"\"\n",
        "    # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "    new_df = pd.DataFrame()\n",
        "    new_df['id'] = df_org['id']\n",
        "    new_df['label'] = labels.astype(str)\n",
        "    new_df['confidence'] = confidences.astype(str)\n",
        "    new_df['landmarks'] = new_df['label'] + ' ' + new_df['confidence']\n",
        "    del new_df['label'], new_df['confidence']\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def predict_label(model, metric_fc, test_dataset, label_encoder):\n",
        "    submit_file = 'submit_landmark.csv'\n",
        "    \n",
        "    # split df in test_dataset and make loader\n",
        "    loaders = split_dataset(test_dataset, 10)\n",
        "    \n",
        "    # write the header of a submission table\n",
        "    df_header = pd.DataFrame(columns=['id', 'landmarks'])\n",
        "    df_header.to_csv(submit_file, index=False)\n",
        "    \n",
        "    # prediction phase\n",
        "    for i, loader in enumerate(loaders):\n",
        "        get_logger().info('prediction %d / %d' % (i+1, len(loaders)))\n",
        "        model.eval()\n",
        "        with torch.no_grad():    \n",
        "            proba = predict_proba(model, metric_fc, loaders[i])\n",
        "            max_proba = np.max(proba, axis=1)\n",
        "            max_proba_idx = np.argmax(proba, axis=1)\n",
        "            labels = label_encoder.inverse_transform(max_proba_idx)\n",
        "\n",
        "            df_submit = make_df(loader.dataset.df, labels, max_proba)\n",
        "\n",
        "        # write result in appending mode\n",
        "        df_submit.to_csv(submit_file, index=False, header=False, mode='a')\n",
        "        \n",
        "    get_logger().info(\"created submission file\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPUzcAtLDwd7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## main"
      ]
    },
    {
      "metadata": {
        "id": "DKnEDDIBascd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE_TRAIN = 150\n",
        "NUM_WORKERS = 8\n",
        "EPOCHS = 10\n",
        "PRINT_FREQ = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11g95PtYVs8L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### train"
      ]
    },
    {
      "metadata": {
        "id": "JRQ04fG0Dwbc",
        "colab_type": "code",
        "outputId": "cb4cbfce-c674-455b-f0cf-68b67f8f89bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(TRAIN_PATH)\n",
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>landmark_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>97c0a12e07ae8dd5</td>\n",
              "      <td>http://lh4.ggpht.com/-f8xYA5l4apw/RSziSQVaABI/...</td>\n",
              "      <td>6347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>650c989dd3493748</td>\n",
              "      <td>https://lh5.googleusercontent.com/-PUnMrX7oOyA...</td>\n",
              "      <td>12519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>05e63ca9b2cde1f4</td>\n",
              "      <td>http://mw2.google.com/mw-panoramio/photos/medi...</td>\n",
              "      <td>264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>08672eddcb2b7c93</td>\n",
              "      <td>http://lh3.ggpht.com/-9fgSxDYwhHA/SMvGEoltKTI/...</td>\n",
              "      <td>13287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fc49cb32ef7f1e89</td>\n",
              "      <td>http://lh6.ggpht.com/-UGAXxvPbr98/S-jGZbyMIPI/...</td>\n",
              "      <td>4018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                                url  \\\n",
              "0  97c0a12e07ae8dd5  http://lh4.ggpht.com/-f8xYA5l4apw/RSziSQVaABI/...   \n",
              "1  650c989dd3493748  https://lh5.googleusercontent.com/-PUnMrX7oOyA...   \n",
              "2  05e63ca9b2cde1f4  http://mw2.google.com/mw-panoramio/photos/medi...   \n",
              "3  08672eddcb2b7c93  http://lh3.ggpht.com/-9fgSxDYwhHA/SMvGEoltKTI/...   \n",
              "4  fc49cb32ef7f1e89  http://lh6.ggpht.com/-UGAXxvPbr98/S-jGZbyMIPI/...   \n",
              "\n",
              "  landmark_id  \n",
              "0        6347  \n",
              "1       12519  \n",
              "2         264  \n",
              "3       13287  \n",
              "4        4018  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "QYHywf19jfwx",
        "colab_type": "code",
        "outputId": "8c86f16d-bcf3-4b71-e89d-87b2f9a448a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "# must be init_le() before get_exist_image()\n",
        "label_encoder = init_le(df_train)\n",
        "joblib.dump(label_encoder, 'le.pkl')\n",
        "\n",
        "df_train = get_exist_image(df_train, TRAIN_IMG_PATH)\n",
        "id_count = df_train['landmark_id'].value_counts()\n",
        "s_count = df_train['landmark_id'].map(id_count)\n",
        "df_train = df_train[s_count > 1]\n",
        "print('more than 1 landmark_id: %d, images: %d' % ((id_count > 1).sum(), df_train.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 256/256 [00:02<00:00, 113.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "exist images: 340748\n",
            "more than 1 landmark_id: 11257, images: 338428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UpFXwjrxDwY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = LandmarkDataset(TRAIN_IMG_PATH, df_train, \n",
        "                                trn_trnsfms, is_train=True, le=label_encoder)\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=BATCH_SIZE_TRAIN,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=True,\n",
        "                          shuffle=True\n",
        "                         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VFP18BuDwQx",
        "colab_type": "code",
        "outputId": "2d4be229-de05-4b43-d46e-8ae69b9cf759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset.df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(338428, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "450xobzQoojo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_dim = 512\n",
        "\n",
        "n_classes = len(label_encoder.classes_)\n",
        "#model = DenseNet(output_neurons=latent_dim, n_classes=len(le.classes_),  dropout_rate=0.5).cuda()    \n",
        "model = ResNet(output_neurons=latent_dim, n_classes=n_classes, dropout_rate=0.5).cuda()    \n",
        "metric_fc = ArcMarginProduct(latent_dim, n_classes, s=60, m=0.5, easy_margin=False).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = FocalLoss(gamma=2)\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = optim.SGD([{'params':model.parameters()}, {'params': metric_fc.parameters()}], \n",
        "                      lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler_step = EPOCHS\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, eta_min=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FXFjd-6xtEmq",
        "colab_type": "code",
        "outputId": "e97e6ce0-69f7-4627-a807-83aeaa2aee94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3918
        }
      },
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch+1, EPOCHS+1):\n",
        "    scheduler.step()\n",
        "    \n",
        "    epoch_loss = train(epoch, model, train_loader, metric_fc, criterion, optimizer)\n",
        "    \n",
        "    save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'metric_fc': metric_fc.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict()\n",
        "    }, True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]2019-04-23 15:13:09,894:main:[Start] epoch: 1\n",
            "[INFO]2019-04-23 15:13:09,898:main:lr: 0.001000\n",
            "  0%|          | 0/2256 [00:00<?, ?it/s][INFO]2019-04-23 15:13:50,471:main:i: 0 loss: 41.970425 top1: 0.000000 top5: 0.000000\n",
            "  4%|▍         | 100/2256 [09:15<3:13:32,  5.39s/it][INFO]2019-04-23 15:22:27,513:main:i: 100 loss: 37.222351 top1: 3.287129 top5: 3.656766\n",
            "  9%|▉         | 200/2256 [17:14<1:18:29,  2.29s/it][INFO]2019-04-23 15:30:54,140:main:i: 200 loss: 35.025827 top1: 5.791045 top5: 6.487562\n",
            " 13%|█▎        | 300/2256 [25:52<3:15:17,  5.99s/it][INFO]2019-04-23 15:39:04,198:main:i: 300 loss: 33.730509 top1: 7.435216 top5: 8.345515\n",
            " 18%|█▊        | 400/2256 [33:41<1:23:30,  2.70s/it][INFO]2019-04-23 15:47:05,949:main:i: 400 loss: 32.699968 top1: 9.067332 top5: 10.084788\n",
            " 22%|██▏       | 500/2256 [42:14<3:15:11,  6.67s/it][INFO]2019-04-23 15:55:25,921:main:i: 500 loss: 31.919780 top1: 10.222222 top5: 11.296074\n",
            " 27%|██▋       | 600/2256 [49:54<1:11:30,  2.59s/it][INFO]2019-04-23 16:03:16,773:main:i: 600 loss: 31.300533 top1: 11.043816 top5: 12.153078\n",
            " 31%|███       | 700/2256 [58:47<3:31:19,  8.15s/it][INFO]2019-04-23 16:11:58,934:main:i: 700 loss: 30.732071 top1: 11.849738 top5: 12.971945\n",
            " 35%|███▌      | 800/2256 [1:07:00<1:11:55,  2.96s/it][INFO]2019-04-23 16:20:13,985:main:i: 800 loss: 30.244810 top1: 12.515189 top5: 13.676238\n",
            " 40%|███▉      | 900/2256 [1:15:13<2:02:46,  5.43s/it][INFO]2019-04-23 16:28:24,727:main:i: 900 loss: 29.803351 top1: 13.097299 top5: 14.300406\n",
            " 44%|████▍     | 1000/2256 [1:23:08<1:28:56,  4.25s/it][INFO]2019-04-23 16:36:19,834:main:i: 1000 loss: 29.400464 top1: 13.617050 top5: 14.879787\n",
            " 49%|████▉     | 1100/2256 [1:31:17<1:02:35,  3.25s/it][INFO]2019-04-23 16:44:29,808:main:i: 1100 loss: 29.038555 top1: 14.082350 top5: 15.392068\n",
            " 53%|█████▎    | 1200/2256 [1:39:17<1:02:26,  3.55s/it][INFO]2019-04-23 16:52:32,460:main:i: 1200 loss: 28.720762 top1: 14.463503 top5: 15.808494\n",
            " 58%|█████▊    | 1300/2256 [1:47:31<1:41:18,  6.36s/it][INFO]2019-04-23 17:00:43,307:main:i: 1300 loss: 28.388259 top1: 14.903919 top5: 16.301306\n",
            " 62%|██████▏   | 1400/2256 [1:55:20<39:56,  2.80s/it][INFO]2019-04-23 17:08:48,134:main:i: 1400 loss: 28.087847 top1: 15.282417 top5: 16.720913\n",
            " 66%|██████▋   | 1500/2256 [2:03:44<1:18:20,  6.22s/it][INFO]2019-04-23 17:16:55,530:main:i: 1500 loss: 27.810269 top1: 15.614478 top5: 17.109482\n",
            " 71%|███████   | 1600/2256 [2:11:27<28:26,  2.60s/it][INFO]2019-04-23 17:24:51,266:main:i: 1600 loss: 27.524280 top1: 16.001665 top5: 17.555279\n",
            " 75%|███████▌  | 1700/2256 [2:19:35<33:19,  3.60s/it][INFO]2019-04-23 17:32:46,898:main:i: 1700 loss: 27.267702 top1: 16.317852 top5: 17.922400\n",
            " 80%|███████▉  | 1800/2256 [2:27:35<30:33,  4.02s/it][INFO]2019-04-23 17:41:00,165:main:i: 1800 loss: 27.002763 top1: 16.691467 top5: 18.340551\n",
            " 84%|████████▍ | 1900/2256 [2:35:38<25:58,  4.38s/it][INFO]2019-04-23 17:48:50,179:main:i: 1900 loss: 26.764484 top1: 17.019112 top5: 18.707697\n",
            " 89%|████████▊ | 2000/2256 [2:43:35<18:00,  4.22s/it][INFO]2019-04-23 17:57:56,797:main:i: 2000 loss: 26.524863 top1: 17.323671 top5: 19.070465\n",
            " 93%|█████████▎| 2100/2256 [3:03:24<14:42,  5.66s/it][INFO]2019-04-23 18:16:35,832:main:i: 2100 loss: 26.302503 top1: 17.620815 top5: 19.421228\n",
            " 98%|█████████▊| 2200/2256 [3:12:21<02:09,  2.32s/it][INFO]2019-04-23 18:25:55,132:main:i: 2200 loss: 26.091234 top1: 17.912161 top5: 19.757080\n",
            "100%|██████████| 2256/2256 [3:16:49<00:00,  2.26s/it]\n",
            "[INFO]2019-04-23 18:30:00,013:main:Epoch 1/10 train loss 25.983054\n",
            "[INFO]2019-04-23 18:30:00,014:main:Finished updating dataset\n",
            "[INFO]2019-04-23 18:30:08,522:main:[Start] epoch: 2\n",
            "[INFO]2019-04-23 18:30:08,526:main:lr: 0.000978\n",
            "  0%|          | 0/2256 [00:00<?, ?it/s][INFO]2019-04-23 18:30:20,569:main:i: 0 loss: 20.733261 top1: 29.333334 top5: 32.000000\n",
            "  4%|▍         | 100/2256 [03:00<1:02:23,  1.74s/it][INFO]2019-04-23 18:33:11,165:main:i: 100 loss: 20.292457 top1: 25.518152 top5: 28.693069\n",
            "  9%|▉         | 200/2256 [05:56<1:00:24,  1.76s/it][INFO]2019-04-23 18:36:06,752:main:i: 200 loss: 20.114671 top1: 25.847429 top5: 29.048094\n",
            " 13%|█▎        | 300/2256 [08:53<57:42,  1.77s/it][INFO]2019-04-23 18:39:03,830:main:i: 300 loss: 20.042124 top1: 25.993357 top5: 29.277964\n",
            " 18%|█▊        | 400/2256 [11:49<54:26,  1.76s/it][INFO]2019-04-23 18:42:00,043:main:i: 400 loss: 19.933978 top1: 26.184540 top5: 29.454699\n",
            " 22%|██▏       | 500/2256 [14:45<51:37,  1.76s/it][INFO]2019-04-23 18:44:56,277:main:i: 500 loss: 19.847448 top1: 26.332668 top5: 29.618097\n",
            " 27%|██▋       | 600/2256 [17:42<48:44,  1.77s/it][INFO]2019-04-23 18:47:52,393:main:i: 600 loss: 19.737570 top1: 26.564613 top5: 29.922350\n",
            " 31%|███       | 700/2256 [20:38<45:54,  1.77s/it][INFO]2019-04-23 18:50:48,947:main:i: 700 loss: 19.649383 top1: 26.692343 top5: 30.071325\n",
            " 35%|███▌      | 800/2256 [23:34<42:33,  1.75s/it][INFO]2019-04-23 18:53:44,967:main:i: 800 loss: 19.581683 top1: 26.799833 top5: 30.211403\n",
            " 40%|███▉      | 900/2256 [26:30<39:40,  1.76s/it][INFO]2019-04-23 18:56:40,800:main:i: 900 loss: 19.458766 top1: 27.045504 top5: 30.486126\n",
            " 44%|████▍     | 1000/2256 [29:26<36:47,  1.76s/it][INFO]2019-04-23 18:59:36,712:main:i: 1000 loss: 19.356376 top1: 27.180820 top5: 30.691309\n",
            " 49%|████▉     | 1100/2256 [32:22<33:49,  1.76s/it][INFO]2019-04-23 19:02:32,886:main:i: 1100 loss: 19.271278 top1: 27.318195 top5: 30.887074\n",
            " 53%|█████▎    | 1200/2256 [35:18<31:07,  1.77s/it][INFO]2019-04-23 19:05:29,140:main:i: 1200 loss: 19.185223 top1: 27.489315 top5: 31.087427\n",
            " 58%|█████▊    | 1300/2256 [38:15<28:12,  1.77s/it][INFO]2019-04-23 19:08:25,584:main:i: 1300 loss: 19.091911 top1: 27.662310 top5: 31.305662\n",
            " 62%|██████▏   | 1400/2256 [41:12<25:10,  1.76s/it][INFO]2019-04-23 19:11:22,650:main:i: 1400 loss: 19.001326 top1: 27.837734 top5: 31.509398\n",
            " 66%|██████▋   | 1500/2256 [44:08<22:09,  1.76s/it][INFO]2019-04-23 19:14:18,904:main:i: 1500 loss: 18.910653 top1: 28.029757 top5: 31.736174\n",
            " 71%|███████   | 1600/2256 [47:05<19:17,  1.77s/it][INFO]2019-04-23 19:17:15,712:main:i: 1600 loss: 18.823038 top1: 28.228607 top5: 31.957111\n",
            " 75%|███████▌  | 1700/2256 [50:01<16:18,  1.76s/it][INFO]2019-04-23 19:20:12,087:main:i: 1700 loss: 18.741983 top1: 28.361357 top5: 32.135998\n",
            " 80%|███████▉  | 1800/2256 [52:57<13:22,  1.76s/it][INFO]2019-04-23 19:23:08,016:main:i: 1800 loss: 18.660956 top1: 28.516750 top5: 32.326855\n",
            " 84%|████████▍ | 1900/2256 [55:53<10:28,  1.77s/it][INFO]2019-04-23 19:26:04,148:main:i: 1900 loss: 18.579251 top1: 28.647028 top5: 32.502541\n",
            " 89%|████████▊ | 2000/2256 [58:50<07:31,  1.76s/it][INFO]2019-04-23 19:29:00,696:main:i: 2000 loss: 18.500879 top1: 28.781277 top5: 32.678993\n",
            " 93%|█████████▎| 2100/2256 [1:01:46<04:33,  1.76s/it][INFO]2019-04-23 19:31:56,669:main:i: 2100 loss: 18.424224 top1: 28.900524 top5: 32.847534\n",
            " 98%|█████████▊| 2200/2256 [1:04:42<01:38,  1.76s/it][INFO]2019-04-23 19:34:52,636:main:i: 2200 loss: 18.355275 top1: 29.019236 top5: 33.006512\n",
            "100%|██████████| 2256/2256 [1:06:21<00:00,  1.76s/it]\n",
            "[INFO]2019-04-23 19:36:29,915:main:Epoch 2/10 train loss 18.316041\n",
            "[INFO]2019-04-23 19:36:29,917:main:Finished updating dataset\n",
            "[INFO]2019-04-23 19:36:33,801:main:[Start] epoch: 3\n",
            "[INFO]2019-04-23 19:36:33,803:main:lr: 0.000914\n",
            "  0%|          | 0/2256 [00:00<?, ?it/s][INFO]2019-04-23 19:36:46,584:main:i: 0 loss: 16.088968 top1: 36.666668 top5: 38.666668\n",
            "  4%|▍         | 100/2256 [03:06<1:03:12,  1.76s/it][INFO]2019-04-23 19:39:42,198:main:i: 100 loss: 15.365415 top1: 34.059406 top5: 39.313530\n",
            "  9%|▉         | 200/2256 [06:03<1:00:27,  1.76s/it][INFO]2019-04-23 19:42:38,840:main:i: 200 loss: 15.246448 top1: 34.407959 top5: 39.538971\n",
            " 13%|█▎        | 300/2256 [08:59<57:12,  1.75s/it][INFO]2019-04-23 19:45:34,938:main:i: 300 loss: 15.170416 top1: 34.737541 top5: 39.869328\n",
            " 18%|█▊        | 400/2256 [11:55<54:24,  1.76s/it][INFO]2019-04-23 19:48:31,132:main:i: 400 loss: 15.069482 top1: 35.012470 top5: 40.159603\n",
            " 22%|██▏       | 500/2256 [14:51<51:41,  1.77s/it][INFO]2019-04-23 19:51:27,123:main:i: 500 loss: 15.015252 top1: 35.097805 top5: 40.283432\n",
            " 27%|██▋       | 600/2256 [17:47<48:31,  1.76s/it][INFO]2019-04-23 19:54:23,180:main:i: 600 loss: 14.982327 top1: 35.219078 top5: 40.422626\n",
            " 31%|███       | 700/2256 [20:43<45:49,  1.77s/it][INFO]2019-04-23 19:57:19,389:main:i: 700 loss: 14.927858 top1: 35.323822 top5: 40.560150\n",
            " 35%|███▌      | 800/2256 [23:39<42:39,  1.76s/it][INFO]2019-04-23 20:00:15,555:main:i: 800 loss: 14.882474 top1: 35.503952 top5: 40.731586\n",
            " 40%|███▉      | 900/2256 [26:36<39:36,  1.75s/it][INFO]2019-04-23 20:03:11,897:main:i: 900 loss: 14.854245 top1: 35.621902 top5: 40.842026\n",
            " 44%|████▍     | 1000/2256 [29:33<36:56,  1.76s/it][INFO]2019-04-23 20:06:08,623:main:i: 1000 loss: 14.796992 top1: 35.761574 top5: 40.979687\n",
            " 49%|████▉     | 1100/2256 [32:29<33:55,  1.76s/it][INFO]2019-04-23 20:09:04,658:main:i: 1100 loss: 14.762969 top1: 35.835907 top5: 41.062065\n",
            " 53%|█████▎    | 1200/2256 [35:25<30:52,  1.75s/it][INFO]2019-04-23 20:12:00,677:main:i: 1200 loss: 14.702885 top1: 35.976688 top5: 41.215656\n",
            " 58%|█████▊    | 1300/2256 [38:20<28:00,  1.76s/it][INFO]2019-04-23 20:14:56,363:main:i: 1300 loss: 14.656416 top1: 36.088135 top5: 41.361515\n",
            " 62%|██████▏   | 1400/2256 [41:16<25:06,  1.76s/it][INFO]2019-04-23 20:17:52,270:main:i: 1400 loss: 14.621655 top1: 36.196526 top5: 41.464191\n",
            " 66%|██████▋   | 1500/2256 [44:12<22:10,  1.76s/it][INFO]2019-04-23 20:20:48,222:main:i: 1500 loss: 14.584775 top1: 36.275814 top5: 41.550522\n",
            " 71%|███████   | 1600/2256 [47:08<19:19,  1.77s/it][INFO]2019-04-23 20:23:44,468:main:i: 1600 loss: 14.537373 top1: 36.398918 top5: 41.686447\n",
            " 75%|███████▌  | 1700/2256 [50:05<16:18,  1.76s/it][INFO]2019-04-23 20:26:40,653:main:i: 1700 loss: 14.505463 top1: 36.469528 top5: 41.778561\n",
            " 80%|███████▉  | 1800/2256 [53:01<13:24,  1.77s/it][INFO]2019-04-23 20:29:36,659:main:i: 1800 loss: 14.477297 top1: 36.539700 top5: 41.858227\n",
            " 84%|████████▍ | 1900/2256 [55:57<10:26,  1.76s/it][INFO]2019-04-23 20:32:32,969:main:i: 1900 loss: 14.435001 top1: 36.638611 top5: 41.977203\n",
            " 89%|████████▊ | 2000/2256 [58:53<07:30,  1.76s/it][INFO]2019-04-23 20:35:29,250:main:i: 2000 loss: 14.393711 top1: 36.752956 top5: 42.112278\n",
            " 93%|█████████▎| 2100/2256 [1:01:50<04:37,  1.78s/it][INFO]2019-04-23 20:38:25,657:main:i: 2100 loss: 14.358997 top1: 36.830399 top5: 42.202759\n",
            " 98%|█████████▊| 2200/2256 [1:04:46<01:38,  1.77s/it][INFO]2019-04-23 20:41:22,053:main:i: 2200 loss: 14.316115 top1: 36.941998 top5: 42.320461\n",
            "100%|██████████| 2256/2256 [1:06:25<00:00,  1.76s/it]\n",
            "[INFO]2019-04-23 20:42:59,444:main:Epoch 3/10 train loss 14.288962\n",
            "[INFO]2019-04-23 20:42:59,446:main:Finished updating dataset\n",
            "[INFO]2019-04-23 20:43:02,992:main:[Start] epoch: 4\n",
            "[INFO]2019-04-23 20:43:02,994:main:lr: 0.000815\n",
            "  0%|          | 0/2256 [00:00<?, ?it/s][INFO]2019-04-23 20:43:14,676:main:i: 0 loss: 13.262527 top1: 36.000000 top5: 45.333336\n",
            "  4%|▍         | 100/2256 [03:07<1:02:59,  1.75s/it][INFO]2019-04-23 20:46:11,877:main:i: 100 loss: 12.051515 top1: 42.079208 top5: 48.026402\n",
            "  9%|▉         | 200/2256 [06:03<1:00:31,  1.77s/it][INFO]2019-04-23 20:49:08,097:main:i: 200 loss: 11.923053 top1: 42.344944 top5: 48.344944\n",
            " 13%|█▎        | 300/2256 [08:59<57:39,  1.77s/it][INFO]2019-04-23 20:52:04,435:main:i: 300 loss: 11.837464 top1: 42.624588 top5: 48.600224\n",
            " 18%|█▊        | 400/2256 [11:55<54:24,  1.76s/it][INFO]2019-04-23 20:55:00,439:main:i: 400 loss: 11.795219 top1: 42.814632 top5: 48.788033\n",
            " 22%|██▏       | 500/2256 [14:51<51:26,  1.76s/it][INFO]2019-04-23 20:57:56,435:main:i: 500 loss: 11.794991 top1: 42.775780 top5: 48.741184\n",
            " 27%|██▋       | 600/2256 [17:48<48:36,  1.76s/it][INFO]2019-04-23 21:00:52,895:main:i: 600 loss: 11.843094 top1: 42.677757 top5: 48.653355\n",
            " 31%|███       | 700/2256 [20:44<45:47,  1.77s/it][INFO]2019-04-23 21:03:49,023:main:i: 700 loss: 11.843492 top1: 42.692345 top5: 48.646694\n",
            " 35%|███▌      | 800/2256 [23:40<42:47,  1.76s/it][INFO]2019-04-23 21:06:45,134:main:i: 800 loss: 11.829703 top1: 42.714108 top5: 48.677486\n",
            " 40%|███▉      | 900/2256 [26:36<39:48,  1.76s/it][INFO]2019-04-23 21:09:40,880:main:i: 900 loss: 11.801978 top1: 42.772472 top5: 48.771736\n",
            " 44%|████▍     | 1000/2256 [29:31<36:43,  1.75s/it][INFO]2019-04-23 21:12:36,553:main:i: 1000 loss: 11.800305 top1: 42.741924 top5: 48.775894\n",
            " 49%|████▉     | 1100/2256 [32:27<33:57,  1.76s/it][INFO]2019-04-23 21:15:32,469:main:i: 1100 loss: 11.771951 top1: 42.795036 top5: 48.853165\n",
            " 53%|█████▎    | 1200/2256 [35:23<30:58,  1.76s/it][INFO]2019-04-23 21:18:28,291:main:i: 1200 loss: 11.751267 top1: 42.849850 top5: 48.938107\n",
            " 58%|█████▊    | 1300/2256 [38:19<28:05,  1.76s/it][INFO]2019-04-23 21:21:24,037:main:i: 1300 loss: 11.715717 top1: 42.965923 top5: 49.067383\n",
            " 62%|██████▏   | 1400/2256 [41:15<25:07,  1.76s/it][INFO]2019-04-23 21:24:20,179:main:i: 1400 loss: 11.697563 top1: 42.985012 top5: 49.098740\n",
            " 66%|██████▋   | 1500/2256 [44:11<22:06,  1.75s/it][INFO]2019-04-23 21:27:15,839:main:i: 1500 loss: 11.683536 top1: 43.030422 top5: 49.145901\n",
            " 71%|███████   | 1600/2256 [47:06<19:12,  1.76s/it][INFO]2019-04-23 21:30:11,589:main:i: 1600 loss: 11.661243 top1: 43.099731 top5: 49.213825\n",
            " 75%|███████▌  | 1700/2256 [50:02<16:18,  1.76s/it][INFO]2019-04-23 21:33:07,526:main:i: 1700 loss: 11.632427 top1: 43.172253 top5: 49.313347\n",
            " 80%|███████▉  | 1800/2256 [52:58<13:25,  1.77s/it][INFO]2019-04-23 21:36:03,684:main:i: 1800 loss: 11.622379 top1: 43.213402 top5: 49.354065\n",
            " 84%|████████▍ | 1900/2256 [55:54<10:26,  1.76s/it][INFO]2019-04-23 21:38:59,593:main:i: 1900 loss: 11.610894 top1: 43.268105 top5: 49.418198\n",
            " 89%|████████▊ | 2000/2256 [58:51<07:30,  1.76s/it][INFO]2019-04-23 21:41:55,834:main:i: 2000 loss: 11.601061 top1: 43.315342 top5: 49.477596\n",
            " 93%|█████████▎| 2100/2256 [1:01:47<04:33,  1.76s/it][INFO]2019-04-23 21:44:52,021:main:i: 2100 loss: 11.593506 top1: 43.342537 top5: 49.517056\n",
            " 98%|█████████▊| 2200/2256 [1:04:43<01:38,  1.76s/it][INFO]2019-04-23 21:47:47,841:main:i: 2200 loss: 11.568540 top1: 43.406334 top5: 49.601093\n",
            "100%|██████████| 2256/2256 [1:06:21<00:00,  1.76s/it]\n",
            "[INFO]2019-04-23 21:49:24,950:main:Epoch 4/10 train loss 11.563906\n",
            "[INFO]2019-04-23 21:49:24,951:main:Finished updating dataset\n",
            "[INFO]2019-04-23 21:49:28,543:main:[Start] epoch: 5\n",
            "[INFO]2019-04-23 21:49:28,552:main:lr: 0.000689\n",
            "  0%|          | 0/2256 [00:00<?, ?it/s][INFO]2019-04-23 21:49:41,075:main:i: 0 loss: 10.013642 top1: 47.333336 top5: 54.666668\n",
            "  4%|▍         | 100/2256 [03:07<1:02:59,  1.75s/it][INFO]2019-04-23 21:52:38,182:main:i: 100 loss: 9.804679 top1: 47.557755 top5: 53.927391\n",
            "  9%|▉         | 200/2256 [06:04<1:00:19,  1.76s/it][INFO]2019-04-23 21:55:34,338:main:i: 200 loss: 9.737039 top1: 47.386402 top5: 54.212273\n",
            " 13%|█▎        | 300/2256 [08:59<57:28,  1.76s/it][INFO]2019-04-23 21:58:30,250:main:i: 300 loss: 9.678284 top1: 47.692139 top5: 54.462902\n",
            " 18%|█▊        | 400/2256 [11:55<54:29,  1.76s/it][INFO]2019-04-23 22:01:26,291:main:i: 400 loss: 9.711456 top1: 47.842064 top5: 54.532005\n",
            " 22%|██▏       | 500/2256 [14:51<51:16,  1.75s/it][INFO]2019-04-23 22:04:22,096:main:i: 500 loss: 9.720293 top1: 47.989353 top5: 54.650696\n",
            " 27%|██▋       | 600/2256 [17:47<48:23,  1.75s/it][INFO]2019-04-23 22:07:18,040:main:i: 600 loss: 9.705308 top1: 48.067665 top5: 54.745422\n",
            " 31%|███       | 700/2256 [20:43<45:33,  1.76s/it][INFO]2019-04-23 22:10:14,110:main:i: 700 loss: 9.673156 top1: 48.172134 top5: 54.874939\n",
            " 35%|███▌      | 800/2256 [23:39<42:32,  1.75s/it][INFO]2019-04-23 22:13:09,970:main:i: 800 loss: 9.658005 top1: 48.312943 top5: 55.014565\n",
            " 40%|███▉      | 900/2256 [26:35<39:54,  1.77s/it][INFO]2019-04-23 22:16:05,966:main:i: 900 loss: 9.658453 top1: 48.337402 top5: 55.049942\n",
            " 44%|████▍     | 1000/2256 [29:31<36:47,  1.76s/it][INFO]2019-04-23 22:19:01,883:main:i: 1000 loss: 9.649987 top1: 48.377625 top5: 55.111557\n",
            " 49%|████▉     | 1100/2256 [32:27<33:52,  1.76s/it][INFO]2019-04-23 22:21:57,775:main:i: 1100 loss: 9.649489 top1: 48.412960 top5: 55.140175\n",
            " 53%|█████▎    | 1200/2256 [35:23<31:01,  1.76s/it][INFO]2019-04-23 22:24:53,608:main:i: 1200 loss: 9.655542 top1: 48.378021 top5: 55.111855\n",
            " 58%|█████▊    | 1300/2256 [38:19<27:57,  1.75s/it][INFO]2019-04-23 22:27:49,598:main:i: 1300 loss: 9.636550 top1: 48.461182 top5: 55.177555\n",
            " 62%|██████▏   | 1400/2256 [41:15<25:13,  1.77s/it][INFO]2019-04-23 22:30:45,448:main:i: 1400 loss: 9.625080 top1: 48.534382 top5: 55.223412\n",
            " 66%|██████▋   | 1500/2256 [44:10<22:05,  1.75s/it][INFO]2019-04-23 22:33:40,960:main:i: 1500 loss: 9.622326 top1: 48.560959 top5: 55.228069\n",
            " 71%|███████   | 1600/2256 [47:06<19:12,  1.76s/it][INFO]2019-04-23 22:36:36,749:main:i: 1600 loss: 9.618071 top1: 48.608372 top5: 55.274620\n",
            " 75%|███████▌  | 1700/2256 [50:01<16:14,  1.75s/it][INFO]2019-04-23 22:39:32,238:main:i: 1700 loss: 9.604008 top1: 48.650597 top5: 55.322754\n",
            " 80%|███████▉  | 1800/2256 [52:57<13:19,  1.75s/it][INFO]2019-04-23 22:42:27,988:main:i: 1800 loss: 9.593894 top1: 48.688137 top5: 55.364059\n",
            " 84%|████████▍ | 1900/2256 [55:53<10:24,  1.75s/it][INFO]2019-04-23 22:45:23,789:main:i: 1900 loss: 9.579336 top1: 48.756794 top5: 55.441696\n",
            " 89%|████████▊ | 2000/2256 [58:49<07:32,  1.77s/it][INFO]2019-04-23 22:48:19,562:main:i: 2000 loss: 9.573048 top1: 48.790272 top5: 55.479595\n",
            " 93%|█████████▎| 2100/2256 [1:01:45<04:34,  1.76s/it][INFO]2019-04-23 22:51:15,577:main:i: 2100 loss: 9.564048 top1: 48.811993 top5: 55.510075\n",
            " 98%|█████████▊| 2200/2256 [1:04:41<01:38,  1.76s/it][INFO]2019-04-23 22:54:11,354:main:i: 2200 loss: 9.560911 top1: 48.827503 top5: 55.522038\n",
            "100%|██████████| 2256/2256 [1:06:19<00:00,  1.76s/it]\n",
            "[INFO]2019-04-23 22:55:48,583:main:Epoch 5/10 train loss 9.547611\n",
            "[INFO]2019-04-23 22:55:48,584:main:Finished updating dataset\n",
            "[INFO]2019-04-23 22:55:52,136:main:[Start] epoch: 6\n",
            "[INFO]2019-04-23 22:55:52,140:main:lr: 0.000550\n",
            "  0%|          | 0/2256 [00:00<?, ?it/s][INFO]2019-04-23 22:56:03,965:main:i: 0 loss: 8.716971 top1: 53.333340 top5: 57.333336\n",
            "  4%|▍         | 100/2256 [03:06<1:03:07,  1.76s/it][INFO]2019-04-23 22:59:00,849:main:i: 100 loss: 8.082822 top1: 53.280529 top5: 59.960396\n",
            "  9%|▉         | 200/2256 [06:03<1:00:04,  1.75s/it][INFO]2019-04-23 23:01:56,906:main:i: 200 loss: 8.069018 top1: 53.399670 top5: 60.255390\n",
            " 13%|█▎        | 300/2256 [08:58<57:29,  1.76s/it][INFO]2019-04-23 23:04:52,884:main:i: 300 loss: 8.116249 top1: 53.351055 top5: 60.166115\n",
            " 18%|█▊        | 400/2256 [11:54<54:10,  1.75s/it][INFO]2019-04-23 23:07:48,850:main:i: 400 loss: 8.063837 top1: 53.391525 top5: 60.234417\n",
            " 22%|██▏       | 500/2256 [14:51<51:28,  1.76s/it][INFO]2019-04-23 23:10:44,936:main:i: 500 loss: 8.084002 top1: 53.332001 top5: 60.203590\n",
            " 27%|██▋       | 600/2256 [17:47<48:42,  1.76s/it][INFO]2019-04-23 23:13:41,006:main:i: 600 loss: 8.081963 top1: 53.381031 top5: 60.257347\n",
            " 31%|███       | 700/2256 [20:43<45:28,  1.75s/it][INFO]2019-04-23 23:16:37,087:main:i: 700 loss: 8.094850 top1: 53.303848 top5: 60.216831\n",
            " 35%|███▌      | 800/2256 [23:39<42:34,  1.75s/it][INFO]2019-04-23 23:19:33,048:main:i: 800 loss: 8.081211 top1: 53.290054 top5: 60.193092\n",
            " 40%|███▉      | 900/2256 [26:35<40:02,  1.77s/it][INFO]2019-04-23 23:22:29,234:main:i: 900 loss: 8.073751 top1: 53.331112 top5: 60.299667\n",
            " 44%|████▍     | 1000/2256 [29:31<36:50,  1.76s/it][INFO]2019-04-23 23:25:25,221:main:i: 1000 loss: 8.046909 top1: 53.444557 top5: 60.410259\n",
            " 49%|████▉     | 1100/2256 [32:27<33:55,  1.76s/it][INFO]2019-04-23 23:28:21,651:main:i: 1100 loss: 8.044082 top1: 53.475628 top5: 60.422646\n",
            " 53%|█████▎    | 1200/2256 [35:23<31:14,  1.78s/it][INFO]2019-04-23 23:31:17,821:main:i: 1200 loss: 8.040075 top1: 53.459339 top5: 60.417431\n",
            " 58%|█████▊    | 1300/2256 [38:19<27:57,  1.75s/it][INFO]2019-04-23 23:34:13,828:main:i: 1300 loss: 8.036044 top1: 53.490135 top5: 60.459644\n",
            " 62%|██████▏   | 1400/2256 [41:15<25:10,  1.76s/it][INFO]2019-04-23 23:37:09,825:main:i: 1400 loss: 8.032032 top1: 53.466095 top5: 60.435402\n",
            " 66%|██████▋   | 1500/2256 [44:12<22:06,  1.75s/it][INFO]2019-04-23 23:40:05,942:main:i: 1500 loss: 8.027514 top1: 53.498554 top5: 60.468128\n",
            " 71%|███████   | 1600/2256 [47:08<19:27,  1.78s/it][INFO]2019-04-23 23:43:02,212:main:i: 1600 loss: 8.022733 top1: 53.510723 top5: 60.497189\n",
            " 75%|███████▌  | 1700/2256 [50:04<16:17,  1.76s/it][INFO]2019-04-23 23:45:58,295:main:i: 1700 loss: 8.021012 top1: 53.528515 top5: 60.500099\n",
            " 80%|███████▉  | 1800/2256 [53:00<13:25,  1.77s/it][INFO]2019-04-23 23:48:54,668:main:i: 1800 loss: 8.023639 top1: 53.509533 top5: 60.510830\n",
            " 84%|████████▍ | 1900/2256 [55:56<10:26,  1.76s/it][INFO]2019-04-23 23:51:50,693:main:i: 1900 loss: 8.026903 top1: 53.493599 top5: 60.504295\n",
            " 89%|████████▊ | 2000/2256 [58:53<07:29,  1.76s/it][INFO]2019-04-23 23:54:46,949:main:i: 2000 loss: 8.012618 top1: 53.542896 top5: 60.565052\n",
            " 93%|█████████▎| 2100/2256 [1:01:48<04:34,  1.76s/it][INFO]2019-04-23 23:57:42,578:main:i: 2100 loss: 8.009759 top1: 53.537045 top5: 60.552116\n",
            " 98%|█████████▊| 2200/2256 [1:04:44<01:39,  1.77s/it][INFO]2019-04-24 00:00:38,695:main:i: 2200 loss: 8.005714 top1: 53.565048 top5: 60.578526\n",
            "100%|██████████| 2256/2256 [1:06:23<00:00,  1.76s/it]\n",
            "[INFO]2019-04-24 00:02:15,779:main:Epoch 6/10 train loss 8.003902\n",
            "[INFO]2019-04-24 00:02:15,780:main:Finished updating dataset\n",
            "[INFO]2019-04-24 00:02:19,138:main:[Start] epoch: 7\n",
            "[INFO]2019-04-24 00:02:19,139:main:lr: 0.000411\n",
            "  0%|          | 0/2256 [00:00<?, ?it/s][INFO]2019-04-24 00:02:30,382:main:i: 0 loss: 5.999015 top1: 58.000000 top5: 64.666672\n",
            "  4%|▍         | 100/2256 [03:06<1:02:56,  1.75s/it][INFO]2019-04-24 00:05:27,188:main:i: 100 loss: 7.047932 top1: 56.600658 top5: 63.240925\n",
            "  9%|▉         | 200/2256 [06:02<1:00:11,  1.76s/it][INFO]2019-04-24 00:08:23,119:main:i: 200 loss: 7.024446 top1: 56.756222 top5: 63.562191\n",
            " 13%|█▎        | 300/2256 [08:58<57:32,  1.77s/it][INFO]2019-04-24 00:11:19,161:main:i: 300 loss: 6.916072 top1: 57.167221 top5: 64.090813\n",
            " 18%|█▊        | 400/2256 [11:54<54:18,  1.76s/it][INFO]2019-04-24 00:14:15,516:main:i: 400 loss: 6.881616 top1: 57.409813 top5: 64.332504\n",
            " 22%|██▏       | 500/2256 [14:50<51:39,  1.77s/it][INFO]2019-04-24 00:17:11,717:main:i: 500 loss: 6.863918 top1: 57.461079 top5: 64.428474\n",
            " 27%|██▋       | 600/2256 [17:46<48:22,  1.75s/it][INFO]2019-04-24 00:20:07,675:main:i: 600 loss: 6.857349 top1: 57.489738 top5: 64.461449\n",
            " 31%|███       | 700/2256 [20:42<45:39,  1.76s/it][INFO]2019-04-24 00:23:03,603:main:i: 700 loss: 6.858420 top1: 57.456966 top5: 64.440323\n",
            " 35%|███▌      | 800/2256 [23:38<42:31,  1.75s/it][INFO]2019-04-24 00:25:59,685:main:i: 800 loss: 6.853961 top1: 57.509781 top5: 64.488556\n",
            " 40%|███▉      | 900/2256 [26:34<39:52,  1.76s/it][INFO]2019-04-24 00:28:55,852:main:i: 900 loss: 6.856567 top1: 57.487236 top5: 64.483162\n",
            " 44%|████▍     | 1000/2256 [29:30<36:56,  1.76s/it][INFO]2019-04-24 00:31:51,834:main:i: 1000 loss: 6.858069 top1: 57.487183 top5: 64.483521\n",
            " 49%|████▉     | 1100/2256 [32:26<33:46,  1.75s/it][INFO]2019-04-24 00:34:47,867:main:i: 1100 loss: 6.847166 top1: 57.498638 top5: 64.515892\n",
            " 53%|█████▎    | 1200/2256 [35:23<30:52,  1.75s/it][INFO]2019-04-24 00:37:44,003:main:i: 1200 loss: 6.844517 top1: 57.514851 top5: 64.549545\n",
            " 58%|█████▊    | 1300/2256 [38:19<28:03,  1.76s/it][INFO]2019-04-24 00:40:40,114:main:i: 1300 loss: 6.851730 top1: 57.468613 top5: 64.494492\n",
            " 62%|██████▏   | 1400/2256 [41:15<25:11,  1.77s/it][INFO]2019-04-24 00:43:36,156:main:i: 1400 loss: 6.836007 top1: 57.551273 top5: 64.572449\n",
            " 66%|██████▋   | 1500/2256 [44:11<22:06,  1.75s/it][INFO]2019-04-24 00:46:32,333:main:i: 1500 loss: 6.838000 top1: 57.550964 top5: 64.589378\n",
            " 71%|███████   | 1600/2256 [47:07<19:16,  1.76s/it][INFO]2019-04-24 00:49:28,311:main:i: 1600 loss: 6.831998 top1: 57.585258 top5: 64.606285\n",
            " 75%|███████▌  | 1700/2256 [50:03<16:16,  1.76s/it][INFO]2019-04-24 00:52:24,208:main:i: 1700 loss: 6.823507 top1: 57.622185 top5: 64.651382\n",
            " 80%|███████▉  | 1800/2256 [52:59<13:23,  1.76s/it][INFO]2019-04-24 00:55:20,331:main:i: 1800 loss: 6.827202 top1: 57.630207 top5: 64.659637\n",
            " 84%|████████▍ | 1900/2256 [55:55<10:28,  1.76s/it][INFO]2019-04-24 00:58:16,399:main:i: 1900 loss: 6.834131 top1: 57.611782 top5: 64.639313\n",
            " 89%|████████▊ | 2000/2256 [58:51<07:32,  1.77s/it][INFO]2019-04-24 01:01:12,423:main:i: 2000 loss: 6.826308 top1: 57.633850 top5: 64.683990\n",
            " 93%|█████████▎| 2100/2256 [1:01:47<04:33,  1.75s/it][INFO]2019-04-24 01:04:08,380:main:i: 2100 loss: 6.822868 top1: 57.658577 top5: 64.706329\n",
            " 98%|█████████▊| 2200/2256 [1:04:43<01:38,  1.76s/it][INFO]2019-04-24 01:07:04,212:main:i: 2200 loss: 6.827772 top1: 57.631989 top5: 64.669701\n",
            "100%|██████████| 2256/2256 [1:06:21<00:00,  1.76s/it]\n",
            "[INFO]2019-04-24 01:08:41,268:main:Epoch 7/10 train loss 6.824498\n",
            "[INFO]2019-04-24 01:08:41,269:main:Finished updating dataset\n",
            "[INFO]2019-04-24 01:08:44,512:main:[Start] epoch: 8\n",
            "[INFO]2019-04-24 01:08:44,516:main:lr: 0.000285\n",
            "  0%|          | 0/2256 [00:00<?, ?it/s][INFO]2019-04-24 01:08:57,326:main:i: 0 loss: 6.434829 top1: 61.333336 top5: 66.666672\n",
            "  4%|▍         | 100/2256 [03:07<1:03:14,  1.76s/it][INFO]2019-04-24 01:11:54,131:main:i: 100 loss: 6.023828 top1: 60.534653 top5: 67.676567\n",
            "  9%|▉         | 200/2256 [06:04<1:00:27,  1.76s/it][INFO]2019-04-24 01:14:50,305:main:i: 200 loss: 5.889958 top1: 60.875622 top5: 68.079605\n",
            " 13%|█▎        | 300/2256 [09:00<57:37,  1.77s/it][INFO]2019-04-24 01:17:46,403:main:i: 300 loss: 5.897191 top1: 60.770767 top5: 68.077522\n",
            " 18%|█▊        | 400/2256 [11:56<54:16,  1.75s/it][INFO]2019-04-24 01:20:42,295:main:i: 400 loss: 5.899128 top1: 60.706570 top5: 68.069832\n",
            " 22%|██▏       | 500/2256 [14:52<51:32,  1.76s/it][INFO]2019-04-24 01:23:38,349:main:i: 500 loss: 5.919166 top1: 60.729206 top5: 68.006653\n",
            " 27%|██▋       | 600/2256 [17:48<48:46,  1.77s/it][INFO]2019-04-24 01:26:34,432:main:i: 600 loss: 5.915753 top1: 60.796448 top5: 68.004433\n",
            " 31%|███       | 700/2256 [20:44<45:32,  1.76s/it][INFO]2019-04-24 01:29:30,553:main:i: 700 loss: 5.927903 top1: 60.748451 top5: 67.971466\n",
            " 35%|███▌      | 800/2256 [23:40<42:42,  1.76s/it][INFO]2019-04-24 01:32:26,391:main:i: 800 loss: 5.939982 top1: 60.656681 top5: 67.875160\n",
            " 40%|███▉      | 900/2256 [26:36<39:41,  1.76s/it][INFO]2019-04-24 01:35:22,647:main:i: 900 loss: 5.930437 top1: 60.733257 top5: 67.906769\n",
            " 44%|████▍     | 1000/2256 [29:32<36:54,  1.76s/it][INFO]2019-04-24 01:38:18,858:main:i: 1000 loss: 5.934436 top1: 60.747921 top5: 67.888779\n",
            " 49%|████▉     | 1100/2256 [32:28<34:11,  1.77s/it][INFO]2019-04-24 01:41:15,020:main:i: 1100 loss: 5.931985 top1: 60.780502 top5: 67.911598\n",
            " 53%|█████▎    | 1200/2256 [35:24<31:04,  1.77s/it][INFO]2019-04-24 01:44:11,279:main:i: 1200 loss: 5.931944 top1: 60.761036 top5: 67.880104\n",
            " 58%|█████▊    | 1300/2256 [38:21<28:06,  1.76s/it][INFO]2019-04-24 01:47:07,596:main:i: 1300 loss: 5.922380 top1: 60.802971 top5: 67.929794\n",
            " 62%|██████▏   | 1400/2256 [41:17<25:02,  1.76s/it][INFO]2019-04-24 01:50:03,930:main:i: 1400 loss: 5.920207 top1: 60.813229 top5: 67.939568\n",
            " 66%|██████▋   | 1500/2256 [44:13<22:09,  1.76s/it][INFO]2019-04-24 01:52:59,804:main:i: 1500 loss: 5.916136 top1: 60.825226 top5: 67.967133\n",
            " 71%|███████   | 1600/2256 [47:09<19:18,  1.77s/it][INFO]2019-04-24 01:55:55,815:main:i: 1600 loss: 5.911805 top1: 60.842808 top5: 67.989594\n",
            " 75%|███████▌  | 1700/2256 [50:05<16:22,  1.77s/it][INFO]2019-04-24 01:58:51,689:main:i: 1700 loss: 5.919173 top1: 60.839897 top5: 67.961594\n",
            " 80%|███████▉  | 1800/2256 [53:01<13:22,  1.76s/it][INFO]2019-04-24 02:01:47,690:main:i: 1800 loss: 5.919121 top1: 60.859524 top5: 67.990379\n",
            " 84%|████████▍ | 1900/2256 [55:57<10:25,  1.76s/it][INFO]2019-04-24 02:04:44,133:main:i: 1900 loss: 5.920238 top1: 60.865158 top5: 68.004906\n",
            " 89%|████████▊ | 2000/2256 [58:53<07:28,  1.75s/it][INFO]2019-04-24 02:07:39,804:main:i: 2000 loss: 5.926303 top1: 60.843246 top5: 67.989670\n",
            " 93%|█████████▎| 2100/2256 [1:01:49<04:33,  1.75s/it][INFO]2019-04-24 02:10:35,450:main:i: 2100 loss: 5.926835 top1: 60.873554 top5: 68.013962\n",
            " 98%|█████████▊| 2200/2256 [1:04:45<01:38,  1.77s/it][INFO]2019-04-24 02:13:31,648:main:i: 2200 loss: 5.922533 top1: 60.884148 top5: 68.021202\n",
            "100%|██████████| 2256/2256 [1:06:23<00:00,  1.76s/it]\n",
            "[INFO]2019-04-24 02:15:08,796:main:Epoch 8/10 train loss 5.925248\n",
            "[INFO]2019-04-24 02:15:08,798:main:Finished updating dataset\n",
            "[INFO]2019-04-24 02:15:12,598:main:[Start] epoch: 9\n",
            "[INFO]2019-04-24 02:15:12,605:main:lr: 0.000186\n",
            "  0%|          | 0/2256 [00:00<?, ?it/s][INFO]2019-04-24 02:15:24,727:main:i: 0 loss: 6.857934 top1: 58.000000 top5: 64.666672\n",
            "  4%|▍         | 100/2256 [03:07<1:03:00,  1.75s/it][INFO]2019-04-24 02:18:21,986:main:i: 100 loss: 5.255184 top1: 63.419140 top5: 70.396042\n",
            "  9%|▉         | 200/2256 [06:03<1:00:13,  1.76s/it][INFO]2019-04-24 02:21:17,842:main:i: 200 loss: 5.291063 top1: 63.290218 top5: 70.404648\n",
            " 13%|█▎        | 300/2256 [08:59<57:43,  1.77s/it][INFO]2019-04-24 02:24:13,871:main:i: 300 loss: 5.288402 top1: 63.379848 top5: 70.378738\n",
            " 15%|█▍        | 332/2256 [09:55<56:27,  1.76s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NuAkY8UDk71k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_checkpoint({\n",
        "    'epoch': epoch,\n",
        "    'state_dict': model.state_dict(),\n",
        "    'metric_fc': metric_fc.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'scheduler': scheduler.state_dict()\n",
        "}, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xc807qKVw6hU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### prediction"
      ]
    },
    {
      "metadata": {
        "id": "7vpb33qAj3MC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_all = pd.read_csv(TEST_PATH)\n",
        "df_test_all.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbK7LyNrj8t4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test = get_exist_image(df_test_all, TEST_IMG_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kYrRy4RScvlS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dataset = LandmarkDataset(TEST_IMG_PATH, df_test, tst_trnsfms, is_train=False)\n",
        "label_encoder = joblib.load('analysis/landmark/models/le.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Gvqw8VvhWtO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_TBDnxiGvDd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_dim = 512\n",
        "\n",
        "n_classes = len(label_encoder.classes_)\n",
        "#model = DenseNet(output_neurons=latent_dim, n_classes=len(le.classes_),  dropout_rate=0.5).cuda()    \n",
        "model = ResNet(output_neurons=latent_dim, n_classes=n_classes, dropout_rate=0.5).cuda()    \n",
        "metric_fc = ArcMarginProduct(latent_dim, n_classes, s=60, m=0.5, easy_margin=False).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = FocalLoss(gamma=2)\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = optim.SGD([{'params':model.parameters()}, {'params': metric_fc.parameters()}], \n",
        "                      lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler_step = 200\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, eta_min=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIQ0_AanWPE7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create model object before following statement\n",
        "start_epoch, model, metric_fc, optimizer, scheduler = load_checkpoint(model, \n",
        "                                                                      metric_fc,\n",
        "                                                                      optimizer,\n",
        "                                                                      scheduler, \n",
        "                                                                      'analysis/landmark/models/best_model.pth')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DxxcCuStBbIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_label(model, metric_fc, test_dataset, label_encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nci2ykdwExk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub = pd.read_csv('submit_landmark.csv')\n",
        "df_sub.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5aTEO9kcipx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_all.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDEx58-1cshM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "epJCM5ZLhiaf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_all.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nfOhWMPpnvog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2 = df_test_all.merge(df_sub, how='left', on='id')[['id', 'landmarks']]\n",
        "df_sub2.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wG-pXdfxn-_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-gb8p20oCf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2['landmarks'].fillna('', inplace=True)\n",
        "df_sub2['landmarks'].isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kyl1lq6NoS70",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2.to_csv('submit_landmark2.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t513LvlOpHT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}