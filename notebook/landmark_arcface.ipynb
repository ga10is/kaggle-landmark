{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "landmark_arcface.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "FMjYTvhlLpwB",
        "V6o7V7DKDAsl",
        "nG4BvN4HDGcf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Viwm7MTHOG-x",
        "colab_type": "code",
        "outputId": "48b36023-87de-488d-e534-5a2db9738715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BHvf6kqSOdzJ",
        "colab_type": "code",
        "outputId": "7683d496-a48c-4024-acca-2dfb6becd211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd \"/gdrive/My Drive\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5wCSnlo3N4Y7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "import joblib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WlhACLidNimS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (256, 256)\n",
        "INPUT = 'analysis/landmark/data/raw/'\n",
        "INDEX_PATH = INPUT + 'index.csv'\n",
        "TRAIN_PATH = INPUT + 'train.csv'\n",
        "TEST_PATH = INPUT + 'test.csv'\n",
        "TRAIN_IMG_PATH = INPUT + 'train/'\n",
        "TEST_IMG_PATH = INPUT + 'test/'\n",
        "INDEX_IMG_PATH = INPUT + 'index/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMjYTvhlLpwB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## utility"
      ]
    },
    {
      "metadata": {
        "id": "BkejDHUXLsj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def debug_deco(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print('--start--')\n",
        "        from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "        func(*args, **kwargs)\n",
        "        print('--end--')\n",
        "    return wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V6o7V7DKDAsl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## logging"
      ]
    },
    {
      "metadata": {
        "id": "3xVYg7fADCcP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "def create_logger(log_file_name):\n",
        "    logger_ = logging.getLogger('main')\n",
        "    logger_.setLevel(logging.DEBUG)\n",
        "    #fh = logging.FileHandler('whale.log')\n",
        "    fh = logging.handlers.RotatingFileHandler(log_file_name, maxBytes=100000, backupCount=8)\n",
        "    fh.setLevel(logging.DEBUG)\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    ch.setFormatter(formatter)\n",
        "    # add the handlers to the logger\n",
        "    logger_.addHandler(fh)\n",
        "    logger_.addHandler(ch)\n",
        "\n",
        "\n",
        "def get_logger():\n",
        "    return logging.getLogger('main')\n",
        "\n",
        "create_logger('landmark.log')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iny_-XckgI5s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "uq2rJ-E6gRuD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "def get_exist_image(_df, _image_folder):\n",
        "    \"\"\"\n",
        "    create dataframe of exist images in folder\n",
        "    \"\"\"\n",
        "    exist_images = get_image_ids(_image_folder)\n",
        "    df_exist = _df[_df['id'].isin(exist_images)]\n",
        "    print('exist images: %d' % len(exist_images))\n",
        "    return df_exist\n",
        "\n",
        "def assert_exist_image(df, image_folder):\n",
        "    exist_images = set(get_image_ids(image_folder))\n",
        "    df_image = set(df['id'].values)\n",
        "    print(len(exist_images))\n",
        "    print(len(df_image))\n",
        "    assert (exist_images == df_image), 'There are not all images in the \"image_folder\"'\n",
        "\n",
        "\n",
        "def get_image_ids_from_subdir(_dir_path, _sub_dir):\n",
        "    sub_dir_path = os.path.join(_dir_path, _sub_dir)\n",
        "    image_ids = [image_file.split('.')[0] for image_file in os.listdir(sub_dir_path)]\n",
        "    return image_ids\n",
        "\n",
        "\n",
        "def get_image_ids(dir_path):\n",
        "    result = []\n",
        "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
        "        futures = []\n",
        "        for sub_dir in os.listdir(dir_path):\n",
        "            futures.append(\n",
        "                executor.submit(get_image_ids_from_subdir, dir_path, sub_dir))\n",
        "\n",
        "        for future in tqdm(futures):\n",
        "            result.extend(future.result())\n",
        "    return result\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "\n",
        "def move_to_folder(dir_path):\n",
        "    for file in tqdm(os.listdir(dir_path)):\n",
        "        if(file[-4:] == '.jpg'):\n",
        "            # move image\n",
        "            sub_dir = file[0:2]\n",
        "            sub_dir_path = os.path.join(dir_path, sub_dir)\n",
        "            old_path = os.path.join(dir_path, file)\n",
        "            new_path = os.path.join(dir_path, sub_dir, file)\n",
        "            \n",
        "            os.makedirs(sub_dir_path, exist_ok=True)\n",
        "            \n",
        "            shutil.move(old_path, new_path)\n",
        "        else:\n",
        "            print('There is a file which is not image: %s' % file)\n",
        "            \n",
        "\n",
        "def init_le(_df):\n",
        "    ids = _df['landmark_id'].values.tolist()\n",
        "    le = LabelEncoder()\n",
        "    le.fit(ids)\n",
        "    return le"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uo7DwF98rCOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# move_to_folder(TEST_IMG_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nG4BvN4HDGcf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## nn"
      ]
    },
    {
      "metadata": {
        "id": "rPsB83a0AHUO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def imshow(img):\n",
        "    #print(type(img))\n",
        "    img = img * 0.23 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    #print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def save_checkpoint(state, is_best, fpath='checkpoint.pth'):\n",
        "    torch.save(state, fpath)\n",
        "    if is_best:\n",
        "        torch.save(state, 'best_model.pth')\n",
        "        \n",
        "def load_checkpoint(_model, \n",
        "                    _metric_fc,\n",
        "                    _optimizer, \n",
        "                    _scheduler, \n",
        "                    fpath):\n",
        "    checkpoint = torch.load(fpath)\n",
        "    _epoch = checkpoint['epoch']\n",
        "    _model.load_state_dict(checkpoint['state_dict'])\n",
        "    _metric_fc.load_state_dict(checkpoint['metric_fc'])\n",
        "    _optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    _scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    \n",
        "    return _epoch, _model, _metric_fc, _optimizer, _scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNimEgEnBu-j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_trnsfms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=(-30,30), shear=(-30,30)),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "tst_trnsfms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-glVaazC0kX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, output_neurons, n_classes, dropout_rate):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.resnet = torchvision.models.resnet34(pretrained=True)        \n",
        "        #self.resnet = torchvision.models.resnet18(pretrained=True)\n",
        "        self.norm1 = nn.BatchNorm1d(512)\n",
        "        self.drop1 = nn.Dropout(dropout_rate)\n",
        "        # FC\n",
        "        self.fc = nn.Linear(512, output_neurons)\n",
        "        self.norm2 = nn.BatchNorm1d(output_neurons)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "\n",
        "        x = self.resnet.layer1(x)\n",
        "        x = self.resnet.layer2(x)\n",
        "        x = self.resnet.layer3(x)\n",
        "        x = self.resnet.layer4(x)\n",
        "        # GAP\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.norm1(x)\n",
        "        x = self.drop1(x)\n",
        "        # FC\n",
        "        x = self.fc(x)\n",
        "        x = self.norm2(x)\n",
        "        #x = l2_norm(x)\n",
        "        return x\n",
        "    \n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, output_neurons, n_classes, dropout_rate):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.densenet_features = torchvision.models.densenet121(pretrained=True).features\n",
        "        self.norm1 = nn.BatchNorm1d(1024)\n",
        "        self.drop1 = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(1024, output_neurons)\n",
        "        self.norm2 = nn.BatchNorm1d(output_neurons)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = self.densenet_features(x)\n",
        "        x = F.relu(features, inplace=True)\n",
        "        # GAP\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).view(features.size(0), -1)\n",
        "        x = self.norm1(x)\n",
        "        x = self.drop1(x)\n",
        "        # FC\n",
        "        x = self.fc(x)\n",
        "        x = self.norm2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O20ikHNLDZzK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import math\n",
        "\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    r\"\"\"Implement of large margin arc distance: :\n",
        "        Args:\n",
        "            in_features: size of each input sample\n",
        "            out_features: size of each output sample\n",
        "            s: norm of input feature\n",
        "            m: margin\n",
        "\n",
        "            cos(theta + m)\n",
        "        \"\"\"\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, input, label=None):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        if label is None:\n",
        "            return cosine\n",
        "        \n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
        "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
        "        output *= self.s\n",
        "        #print(output[0])\n",
        "\n",
        "        return output\n",
        "    \n",
        "class FocalBinaryLoss(nn.Module):\n",
        "    def __init__(self, gamma=0):\n",
        "        super(FocalBinaryLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        p = torch.sigmoid(input)        \n",
        "        loss = torch.mean(-1 * target * torch.pow(1-p, self.gamma) * torch.log(p + 1e-10) +\n",
        "                          -1 * (1-target) * torch.pow(p, self.gamma) * torch.log(1-p + 1e-10)) * 4\n",
        "        return loss\n",
        "    \n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=0, eps=1e-7):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.eps = eps\n",
        "        self.ce = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logp = self.ce(input, target)\n",
        "        p = torch.exp(-logp)\n",
        "        loss = (1 - p) ** self.gamma * logp\n",
        "        return loss.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17SUiMiNDstc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ]
    },
    {
      "metadata": {
        "id": "LVGsNqjGDvXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "class LandmarkDataset(Dataset):\n",
        "    def __init__(self, image_folder, df, transform, is_train, le=None):\n",
        "        self.image_folder = image_folder  \n",
        "        self.transform = transform      \n",
        "        self.df = df\n",
        "        self.is_train = is_train\n",
        "        if is_train:\n",
        "            if le is None:\n",
        "                raise ValueError(\n",
        "                    'Argument \"le\" must not be None when \"is_train\" is True.')\n",
        "            self.le = le\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = '%s.jpg' % self.df.iloc[idx]['id']                   \n",
        "        img = self.__get_image(img_name)\n",
        "        label = None\n",
        "        if self.is_train:\n",
        "            id = self.df.iloc[idx]['landmark_id']\n",
        "            label = torch.tensor(self.le.transform([id]))\n",
        "        else:\n",
        "            label = -1\n",
        "        return img, label\n",
        "    \n",
        "    def __get_image(self, img_name):           \n",
        "        img = self.__load_image(img_name)\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __load_image(self, img_name):\n",
        "        \"\"\"\n",
        "        load images and bound boxing\n",
        "        \"\"\"\n",
        "        sub_folder = img_name[0:2]\n",
        "        path = os.path.join(self.image_folder, sub_folder, img_name)\n",
        "        # load images\n",
        "        img = Image.open(path).convert('RGB')               \n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b5BNMPbCDwod",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## metrics"
      ]
    },
    {
      "metadata": {
        "id": "4uWRFkm3DwlJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def GAP_vector(pred, conf, true, return_x=False):\n",
        "    '''\n",
        "    Compute Global Average Precision (aka micro AP), the metric for the\n",
        "    Google Landmark Recognition competition.\n",
        "    This function takes predictions, labels and confidence scores as vectors.\n",
        "    In both predictions and ground-truth, use None/np.nan for \"no label\".\n",
        "\n",
        "    Args:\n",
        "        pred: vector of integer-coded predictions\n",
        "        conf: vector of probability or confidence scores for pred\n",
        "        true: vector of integer-coded labels for ground truth\n",
        "        return_x: also return the data frame used in the calculation\n",
        "\n",
        "    Returns:\n",
        "        GAP score\n",
        "    '''\n",
        "    x = pd.DataFrame({'pred': pred, 'conf': conf, 'true': true})\n",
        "    x.sort_values('conf', ascending=False, inplace=True, na_position='last')\n",
        "    x['correct'] = (x.true == x.pred).astype(int)\n",
        "    x['prec_k'] = x.correct.cumsum() / (np.arange(len(x)) + 1)\n",
        "    x['term'] = x.prec_k * x.correct\n",
        "    gap = x.term.sum() / x.true.count()\n",
        "    if return_x:\n",
        "        return gap, x\n",
        "    else:\n",
        "        return gap\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ngFQKdh_DwjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## train_valid"
      ]
    },
    {
      "metadata": {
        "id": "srPw7PReDwgm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch, \n",
        "          model,\n",
        "          loader,\n",
        "          metric_fc,\n",
        "          criterion,\n",
        "          optimizer):\n",
        "    loss_meter = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    \n",
        "    get_logger().info('[Start] epoch: %d' % epoch)\n",
        "    get_logger().info('lr: %f' % optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        \n",
        "    # train phase\n",
        "    model.train()\n",
        "    for i, data in enumerate(tqdm(loader)):\n",
        "        img, label = data                \n",
        "        img, label = img.cuda(), label.cuda().long()        \n",
        "        #label = label.squeeze() # (batch_size, 1) -> (batch_size,)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # forward                      \n",
        "            emb_vec = model(img)\n",
        "            logit = metric_fc(emb_vec, label)\n",
        "            loss = criterion(logit, label.squeeze())\n",
        "\n",
        "            # backward\n",
        "            loss.backward()\n",
        "            optimizer.step()            \n",
        "            \n",
        "        # measure accuracy\n",
        "        prec1, prec5 = accuracy(logit.detach(), label, topk=(1, 5))\n",
        "        loss_meter.update(loss.item(), img.size(0))\n",
        "        top1.update(prec1[0], img.size(0))\n",
        "        top5.update(prec5[0], img.size(0))\n",
        "        \n",
        "        # print\n",
        "        if i % PRINT_FREQ == 0:\n",
        "            print('loss: %f top1: %f top5: %f' % (loss_meter.avg, top1.avg, top5.avg))\n",
        "    get_logger().info(\n",
        "        \"Epoch %d/%d train loss %f\" % (epoch, EPOCHS, loss_meter.avg))\n",
        "\n",
        "    # update pairs of image\n",
        "    get_logger().info('Finished updating dataset')\n",
        "    return loss_meter.avg\n",
        "\n",
        "\n",
        "def validate_arcface(model,\n",
        "                     metric_fc,\n",
        "                     unknown_loader,\n",
        "                     label_encoder\n",
        "                    ):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    # validate phase\n",
        "    model.eval()    \n",
        "    with torch.no_grad():       \n",
        "        # predict latent features of Unknown whales\n",
        "        uk_output = predict_arcface(model, metric_fc, unknown_loader)        \n",
        "        n_predict = 1\n",
        "        for i in range(n_predict-1):\n",
        "            uk_output2 = predict_arcface(model, metric_fc, unknown_loader)\n",
        "            print(uk_output2[:5, :5])\n",
        "            uk_output += uk_output2\n",
        "        uk_output /= n_predict\n",
        "        \n",
        "        df_known = make_df(label_encoder)\n",
        "        mat_distance = pd.DataFrame(data=uk_output.T, \n",
        "                                    columns=unknown_loader.dataset.df['Image'].values, \n",
        "                                    index=df_known['Image'].values)\n",
        "\n",
        "        # compute map@5\n",
        "        score = compute_map5(mat_distance, df_known, unknown_loader.dataset.df)        \n",
        "        #acc = accuracy(uclasses, unknown_loader.dataset.df['Id'].values)\n",
        "        get_logger().info(\"validate score %f\" % (score))\n",
        "\n",
        "    return score, mat_distance\n",
        "\n",
        "def predict_proba(model, metric_fc, loader):\n",
        "    \"\"\"\n",
        "    return Tensor of probability for each class\n",
        "    \"\"\"\n",
        "    outputs = []\n",
        "    for data in tqdm(loader):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            img, _ = data\n",
        "            img = img.cuda()        \n",
        "            output = metric_fc(model(img))\n",
        "            outputs.append(output.detach().cpu().numpy())    \n",
        "    outputs = np.concatenate(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "def split_dataset(dataset, steps):\n",
        "    \"\"\"\n",
        "    split Dataset by steps and create DataLoader.\n",
        "    Parameters\n",
        "    dataset: torch.utils.data.Dataset\n",
        "    steps: int\n",
        "        the number of each dataset    \n",
        "    Returns\n",
        "    list of torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "    # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "    _df = dataset.df\n",
        "    n = _df.shape[0]\n",
        "    loader_list = []\n",
        "    \n",
        "    split_indexes= np.array_split(np.arange(n), steps)\n",
        "    for split_index in split_indexes:\n",
        "        split_df = _df.iloc[split_index]\n",
        "        split_dataset = LandmarkDataset(dataset.image_folder, \n",
        "                                        split_df, \n",
        "                                        dataset.transform, \n",
        "                                        is_train=False)\n",
        "        split_loader = DataLoader(split_dataset,\n",
        "                          batch_size=BATCH_SIZE_TRAIN,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=False,\n",
        "                          shuffle=False\n",
        "                         )\n",
        "        loader_list.append(split_loader)\n",
        "    return loader_list\n",
        "\n",
        "    \n",
        "\n",
        "def make_df(df_org, labels, confidences):\n",
        "    \"\"\"\n",
        "    make dataframe for submission\n",
        "    df_org: pd.DataFrame of shape = [n_samples, more than 1]\n",
        "        dataframe which have id column\n",
        "    labels: ndarray of shape = [n_samples]\n",
        "        array of label(number)\n",
        "    confidences: ndarray of shape = [n_samples]\n",
        "        array of confidence(float)\n",
        "    Returns\n",
        "    pd.DataFrame of shape = [n_samples, 2]\n",
        "        the dataframe has 'id', 'landmarks' columns.\n",
        "    \"\"\"\n",
        "    # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
        "    new_df = pd.DataFrame()\n",
        "    new_df['id'] = df_org['id']\n",
        "    new_df['label'] = labels.astype(str)\n",
        "    new_df['confidence'] = confidences.astype(str)\n",
        "    new_df['landmarks'] = new_df['label'] + ' ' + new_df['confidence']\n",
        "    del new_df['label'], new_df['confidence']\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def predict_label(model, metric_fc, test_dataset, label_encoder):\n",
        "    submit_file = 'submit_landmark.csv'\n",
        "    \n",
        "    # split df in test_dataset and make loader\n",
        "    loaders = split_dataset(test_dataset, 10)\n",
        "    \n",
        "    # write the header of a submission table\n",
        "    df_header = pd.DataFrame(columns=['id', 'landmarks'])\n",
        "    df_header.to_csv(submit_file, index=False)\n",
        "    \n",
        "    # prediction phase\n",
        "    for i, loader in enumerate(loaders):\n",
        "        get_logger().info('prediction %d / %d' % (i+1, len(loaders)))\n",
        "        model.eval()\n",
        "        with torch.no_grad():    \n",
        "            proba = predict_proba(model, metric_fc, loaders[i])\n",
        "            max_proba = np.max(proba, axis=1)\n",
        "            max_proba_idx = np.argmax(proba, axis=1)\n",
        "            labels = label_encoder.inverse_transform(max_proba_idx)\n",
        "\n",
        "            df_submit = make_df(loader.dataset.df, labels, max_proba)\n",
        "\n",
        "        # write result in appending mode\n",
        "        df_submit.to_csv(submit_file, index=False, header=False, mode='a')\n",
        "        \n",
        "    get_logger().info(\"created submission file\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPUzcAtLDwd7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## main"
      ]
    },
    {
      "metadata": {
        "id": "DKnEDDIBascd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE_TRAIN = 100\n",
        "NUM_WORKERS = 8\n",
        "EPOCHS = 10\n",
        "PRINT_FREQ = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11g95PtYVs8L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### train"
      ]
    },
    {
      "metadata": {
        "id": "JRQ04fG0Dwbc",
        "colab_type": "code",
        "outputId": "0098a267-4450-42db-97c3-d49727dcb545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(TRAIN_PATH)\n",
        "df_train.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>landmark_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>97c0a12e07ae8dd5</td>\n",
              "      <td>http://lh4.ggpht.com/-f8xYA5l4apw/RSziSQVaABI/...</td>\n",
              "      <td>6347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>650c989dd3493748</td>\n",
              "      <td>https://lh5.googleusercontent.com/-PUnMrX7oOyA...</td>\n",
              "      <td>12519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>05e63ca9b2cde1f4</td>\n",
              "      <td>http://mw2.google.com/mw-panoramio/photos/medi...</td>\n",
              "      <td>264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>08672eddcb2b7c93</td>\n",
              "      <td>http://lh3.ggpht.com/-9fgSxDYwhHA/SMvGEoltKTI/...</td>\n",
              "      <td>13287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fc49cb32ef7f1e89</td>\n",
              "      <td>http://lh6.ggpht.com/-UGAXxvPbr98/S-jGZbyMIPI/...</td>\n",
              "      <td>4018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                                url  \\\n",
              "0  97c0a12e07ae8dd5  http://lh4.ggpht.com/-f8xYA5l4apw/RSziSQVaABI/...   \n",
              "1  650c989dd3493748  https://lh5.googleusercontent.com/-PUnMrX7oOyA...   \n",
              "2  05e63ca9b2cde1f4  http://mw2.google.com/mw-panoramio/photos/medi...   \n",
              "3  08672eddcb2b7c93  http://lh3.ggpht.com/-9fgSxDYwhHA/SMvGEoltKTI/...   \n",
              "4  fc49cb32ef7f1e89  http://lh6.ggpht.com/-UGAXxvPbr98/S-jGZbyMIPI/...   \n",
              "\n",
              "  landmark_id  \n",
              "0        6347  \n",
              "1       12519  \n",
              "2         264  \n",
              "3       13287  \n",
              "4        4018  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "QYHywf19jfwx",
        "colab_type": "code",
        "outputId": "cf1dcdca-b633-46d0-ec0c-fe3390612e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# must be init_le() before get_exist_image()\n",
        "label_encoder = init_le(df_train)\n",
        "joblib.dump(label_encoder, 'le.pkl')\n",
        "\n",
        "df_train = get_exist_image(df_train, TRAIN_IMG_PATH)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 256/256 [00:02<00:00, 111.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "exist images: 340748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UpFXwjrxDwY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = LandmarkDataset(TRAIN_IMG_PATH, df_train, \n",
        "                                trn_trnsfms, is_train=True, le=label_encoder)\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=BATCH_SIZE_TRAIN,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=True,\n",
        "                          shuffle=True\n",
        "                         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tit0fObpDwWL",
        "colab_type": "code",
        "outputId": "18351aa8-3077-48c9-82b6-fd76046373da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "counting = train_dataset.df['landmark_id'].value_counts()\n",
        "(counting > 1).sum()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11257"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "-VFP18BuDwQx",
        "colab_type": "code",
        "outputId": "863fd2df-6151-42d5-a2ef-d806e253654b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset.df.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(340748, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "450xobzQoojo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_dim = 512\n",
        "\n",
        "n_classes = len(label_encoder.classes_)\n",
        "#model = DenseNet(output_neurons=latent_dim, n_classes=len(le.classes_),  dropout_rate=0.5).cuda()    \n",
        "model = ResNet(output_neurons=latent_dim, n_classes=n_classes, dropout_rate=0.5).cuda()    \n",
        "metric_fc = ArcMarginProduct(latent_dim, n_classes, s=60, m=0.5, easy_margin=False).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = FocalLoss(gamma=2)\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = optim.SGD([{'params':model.parameters()}, {'params': metric_fc.parameters()}], \n",
        "                      lr=5e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler_step = EPOCHS\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, eta_min=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FXFjd-6xtEmq",
        "colab_type": "code",
        "outputId": "d46de658-d87b-4ec3-8ea6-2b4af6a3e6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch+1, EPOCHS+1):\n",
        "    scheduler.step()\n",
        "    \n",
        "    epoch_loss = train(epoch, model, train_loader, metric_fc, criterion, optimizer)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]2019-04-21 01:36:15,344:main:[Start] epoch: 1\n",
            "[INFO]2019-04-21 01:36:15,358:main:lr: 0.005000\n",
            "  0%|          | 1/3407 [00:28<27:02:52, 28.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 41.735565 top1: 0.000000 top5: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 11/3407 [01:00<4:51:12,  5.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 41.048110 top1: 0.000000 top5: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 21/3407 [01:36<3:10:31,  3.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 40.751531 top1: 0.000000 top5: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 31/3407 [02:31<2:50:13,  3.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 40.732540 top1: 0.000000 top5: 0.064516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 41/3407 [03:48<8:32:28,  9.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 40.722708 top1: 0.048780 top5: 0.121951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|▏         | 51/3407 [04:50<7:15:37,  7.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 40.683547 top1: 0.058824 top5: 0.137255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 61/3407 [05:37<4:05:36,  4.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 40.593688 top1: 0.229508 top5: 0.360656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 71/3407 [06:22<2:35:35,  2.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 40.376545 top1: 0.619718 top5: 0.873239\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 81/3407 [07:21<5:55:47,  6.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 40.200332 top1: 0.987654 top5: 1.246914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 91/3407 [08:16<6:03:27,  6.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 40.086018 top1: 1.219780 top5: 1.538462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 101/3407 [08:56<3:30:56,  3.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 39.966215 top1: 1.316832 top5: 1.693069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 111/3407 [09:33<2:13:23,  2.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 39.844146 top1: 1.513514 top5: 1.918919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▎         | 121/3407 [10:22<4:44:30,  5.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 39.793416 top1: 1.570248 top5: 1.975207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 131/3407 [11:08<4:55:39,  5.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 39.682541 top1: 1.709924 top5: 2.160305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 137/3407 [11:27<4:31:14,  4.98s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NuAkY8UDk71k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_checkpoint({\n",
        "    'epoch': epoch,\n",
        "    'state_dict': model.state_dict(),\n",
        "    'metric_fc': metric_fc.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'scheduler': scheduler.state_dict()\n",
        "}, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xc807qKVw6hU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### prediction"
      ]
    },
    {
      "metadata": {
        "id": "7vpb33qAj3MC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_all = pd.read_csv(TEST_PATH)\n",
        "df_test_all.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbK7LyNrj8t4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test = get_exist_image(df_test_all, TEST_IMG_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kYrRy4RScvlS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dataset = LandmarkDataset(TEST_IMG_PATH, df_test, tst_trnsfms, is_train=False)\n",
        "label_encoder = joblib.load('analysis/landmark/models/le.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Gvqw8VvhWtO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_TBDnxiGvDd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_dim = 512\n",
        "\n",
        "n_classes = len(label_encoder.classes_)\n",
        "#model = DenseNet(output_neurons=latent_dim, n_classes=len(le.classes_),  dropout_rate=0.5).cuda()    \n",
        "model = ResNet(output_neurons=latent_dim, n_classes=n_classes, dropout_rate=0.5).cuda()    \n",
        "metric_fc = ArcMarginProduct(latent_dim, n_classes, s=60, m=0.5, easy_margin=False).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = FocalLoss(gamma=2)\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = optim.SGD([{'params':model.parameters()}, {'params': metric_fc.parameters()}], \n",
        "                      lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler_step = 200\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, eta_min=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIQ0_AanWPE7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create model object before following statement\n",
        "start_epoch, model, metric_fc, optimizer, scheduler = load_checkpoint(model, \n",
        "                                                                      metric_fc,\n",
        "                                                                      optimizer,\n",
        "                                                                      scheduler, \n",
        "                                                                      'analysis/landmark/models/best_model.pth')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DxxcCuStBbIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_label(model, metric_fc, test_dataset, label_encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nci2ykdwExk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub = pd.read_csv('submit_landmark.csv')\n",
        "df_sub.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5aTEO9kcipx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_all.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDEx58-1cshM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "epJCM5ZLhiaf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_all.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nfOhWMPpnvog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2 = df_test_all.merge(df_sub, how='left', on='id')[['id', 'landmarks']]\n",
        "df_sub2.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wG-pXdfxn-_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-gb8p20oCf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2['landmarks'].fillna('', inplace=True)\n",
        "df_sub2['landmarks'].isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kyl1lq6NoS70",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_sub2.to_csv('submit_landmark2.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t513LvlOpHT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}